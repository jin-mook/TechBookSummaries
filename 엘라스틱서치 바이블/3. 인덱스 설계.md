
엘라스틱서치의 인덱스는 아주 세부적인 부분까지 설정으로 제어할 수 있습니다. 또한 설정에 따라 동작과 특성이 매우 달라지므로 인덱스 설계에 신경 써야 합니다.

이번 장에서는 소개할 내용은 다음과 같습니다.

- 인덱스 설정 방법 및 매핑
- 필드 타입
- 애널라이저
- 인덱스 템플릿
- 라우팅


---


## 1. 인덱스 설정

인덱스를 생성할 때에는 인덱스의 동작에 관한 설정을 지정할 수 있습니다. 중요한 인덱스 설정 몇 가지를 알아봅시다.
인덱스 설정을 조회하려면 인덱스 이름 뒤에 /\_settings  를 넣어 GET 메서드로 호출하면 됩니다.

```
GET [인덱스 이름]/_settings
```

number_of_shards와 number_of_replicas 값에 주목해야 합니다. 또한 응답에 명시적으로 포함되지는 않지만 refresh_interval 이라는 중요한 설정을 알아보겠습니다.


##### 1) number_of_shards

number_of_shards는 이 인덱스가 데이터를 몇 개의 샤드로 쪼갤 것인지 지정하는 값입니다. 이 값은 매우 신중하게 설계해야 하는데, 한 번 지정하면 reindex 같은 동작을 통해 인덱스를 통째로 재색인하는 등 특별한 작업을 수행하지 않는 한 바꿀 수 없기 때문입니다.

샤드 개수를 어떻게 지정하느냐는 엘라스틱서치 클러스터 전체의 성능에도 큰 영향을 미칩니다. 샤드 하나마다 루씬 인덱스가 하나씩 더 생성된다는 사실과 주 샤드 하나당 복제본 샤드도 늘어난다는 사실을 염두해 두어야 합니다.


![[Pasted image 20240116004356.png]]


> number_of_shards 설정의 기본값은 엘라스틱서치 7 버전부터 1로 변경되었습니다.


##### 2) number_of_replicas

number_of_replicas는 주 샤드 하나당 복제본 샤드를 몇 개 둘 것인지를 지정하는 설정입니다. 엘라스틱서치 클러스터에 몇 개의 노드를 붙일 것이며 어느 정도의 고가용성을 제공할 것인지 등을 고려해서 지정하면 됩니다. 이 값은 인덱스 생성 후에도 동적으로 변경할 수 있습니다.

```
PUT [인덱스 이름]/_settings
{
	[변경할 내용]
}

PUT my_index/_settings
{
	"index.number_of_replicas": 0
}
```


위와 같이 값을 0으로 설정하면 복제본 샤드를 생성하지 않고 주 샤드만 둡니다.

> 복제본 샤드를 생성하지 않는 설정은 주로 대용량의 초기 데이터를 마이그레이션하는 등의 시나리오에서 쓰기 성능을 일시적으로 끌어올리기 위해 사용됩니다.


##### 3) refresh_interval

refresh_interval은 엘라스틱서치가 해당 인덱스를 대상으로 refresh를 얼마나 자주 수행할 것인지를 지정합니다. 엘라스틱서치 인덱스에 색인된 문서는 refresh 되어야 검색 대상이 되기 때문에 중요한 설정입니다.

```
PUT my_index/_settings
{
	"index.refresh_interval": "1s"
}
```

이 값을 명시적으로 설정하지 않으면 매 1초마다 refresh를 수행하며, 마지막으로 검색 쿼리가 들어온 시각을 확인한다. 30초 이상 검색 쿼리가 들어오지 않는 것을 확인하면 다음 첫 검색 쿼리가 들어올 때까지 refresh를 수행하지 않습니다. 이 30초 대기 시간은 index.search.idle.after 설정에서 변경할 수 있습니다.



##### 4) 인덱스 설정을 지정하여 인덱스 생성

```
PUT my_index2
{
	"settings": {
		"number_of_shards": 2,
		"number_of_replicas": 2
	}
}
```



---


## 2. 매핑과 필드 타입

매핑은 문서가 인덱스에 어떻게 색인되고 저장되는지 정의하는 부분입니다. JSON 문서의 각 필드를 어떤 방식으로 분석하고 색인할지, 어떤 타입으로 저장할지 등을 세부적으로 지정할 수 있습니다.

인덱스에 문서가 색인될 때 기존에 매핑 정보를 가지고 있지 않던 새로운 필드가 들어오면 엘라스틱서치는 자동으로 문서의 내용을 보고 적당한 필드 타입을 지정해서 매핑 정보를 생성합니다.

> 필드 타입을 포함한 매핑 설정 내 대부분의 내용은 한 번 지정되면 사실상 변경이 불가능하다는 점입니다. 따라서 서비스 설계와 데이터 설계를 할 때는 매우 신중해야 합니다.


위와 같은 이유로 서비스 운영 환경에서 대용량의 데이터를 처리해야 할 때는 기본적으로 명시적으로 매핑을 지정해서 인덱스를 운영해야 합니다. 매핑을 어떻게 지정하느냐에 따라 서비스 운영 양상이 많이 달라지며 성능의 차이도 큽니다.


#### 필드 타입
| <span style="color:#ff0000">분류</span> | <span style="color:#ff0000">종류</span> |
| ---- | ---- |
| 심플 타입 | text, keyword, date, long, double, boolen, ip 등 |
| 계층 구조를 지원하는 타입 | object, nested 등 |
| 그 외 특수한 타입 | geo_point, geo_shape 등 |

#### 1) 심플 타입

심플 타입은 주로 직관적으로 알기 쉬운 간단한 자료형입니다.

> text, keyword는 문자열을 표현하는 타입인데 매우 중요하기 때문에 이후 다시 설명하겠습니다.



##### 숫자 타입

| <span style="color:#ff0000">종류</span> | <span style="color:#ff0000">설명</span> |
| ---- | ---- |
| long | 64비트 부호 있는 정수 |
| integer | 32비트 부호 있는 정수 |
| short | 16비트 부호 있는 정수 |
| byte | 8비트 부호 있는 정수 |
| double | 64비트 부동소수점 수 |
| float | 32비트 부동소수점 수 |
| half_float | 16비트 부동소수점 수 |
| scaled_float | double 고정 환산 계수로 스케일링하여 long으로 저장되는 부동소수점 수 |

 > 작은 비트를 사용하는 자료형을 고르면 색인과 검색 시 이득이 있다. 다만 저장할 때는 실제 값에 맞춰 최적화되기 때문에 디스크 사용량에는 이득이 없다.
 


##### date 타입

date 타입은 인입되는 데이터의 형식을 format이라는 옵션으로 지정할 수 있습니다. format은 여러 형식으로 지정할 수 있으며 문서가 어떤 형식으로 들어오더라도 엘라스틱서치 내부적으로는 UTC 시간대로 변환하는 과정을 거쳐 epoch milliseconds 형식의 long 숫자로 색인됩니다.

| 종류 | 설명 |
| ---- | ---- |
| epoch_millis | 밀리초 단위로 표현한 epoch 시간 |
| epoch_second | 초 단위로 표현한 epoch 시간 |
| date_time | yyyyMMdd 형태로 표현한 날짜 |
| strict_date_time | yyyy-MM-dd'T'HH:mm:ss.SSSZZ로 표현한 날짜와 시간 |
| date_optional_time | 최소 연 단위의 날짜를 포함해야 하며, 선택적으로 시간 정보도 포함하여 ISO datetime 형태로 표현된 날짜와 시간<br>예) yyyy-MM-dd 또는 yyyy-MM-dd'T'HH:mm:ss.SSSz |
| strict_date_optional_time | date_optional_time과 동일하지만 연, 월, 일이 각각 4자리, 2자리, 2자리임을 보장해야 한다. |

여러 형식을 허용하도록 지정하려면 date 타입 설정에서 format 을 || 문자로 이어 붙이면 됩니다. format의 기본값은 trict_date_optional_time || epoch_millis 입니다.


##### 배열

엘라스틱서치에는 배열을 표현하는 별도의 타입 구분이 없습니다. long 타입의 필드에는 해당 타입에 맞는 숫자만 넣을수도 있고 배열 데이터도 넣을 수 있습니다.
그러나 여러 타입이 혼합된 배열 데이터의 색인을 요청하는 경우 색인은 실패합니다.

> 각 필드에는 지정한 필드 타입에 맞는 한 가지 종류의 데이터만 색인해야 하기 때문입니다.

또한 문서 내 지정한 필드의 값이 질의한 값과 일치하는 검색 쿼리를 사용할 때 배열 자체가 아닌 배열에 존재하는 하나의 값을 검색하더라도 검색에 성공합니다.

엘라스틱서치는 색인 과정에서 데이터가 단일 데이터인지 배열 데이터인지에 상관없이 각 값마다 하나의 독립적인 역색인을 구성하기 때문입니다.


 
#### 2) 계층 구조를 지원하는 타입

필드 하위에 다른 필드가 들어가는 계층 구조의 데이터를 담는 타입으로는 object와 nested가 있습니다. 이 둘은 유사하나 배열을 처리할 때의 동작이 다릅니다.

##### object 타입

엘라스틱서치는 데이터를 평탄화시켜서 저장하게 되는데 object 타입의 데이터는 아래와 같이 평탄화 됩니다.

```
PUT object_test/_doc/2
{
	"spec" : [
		{
			"cores": 12,
			"memory": 128
		},
		{
			"cores": 6,
			"memory": 64
		},
		{
			"cores": 6,
			"memory": 32
		},
	]
}
```

위 인덱스에서 spec.cores 가 6개이며, spec.memory는 128인 문서를 검색해보면 하나도 나오지 않아야 할것 같지만 검색 결과에는 데이터가 존재하게 됩니다.

이는 object 타입의 데이터가 어떻게 평탄화되는지를 알아보면 알 수 있습니다. 내부적으로는 다음과 같이 평탄화됩니다.

```
{
	"spec.cores": [12, 6, 6],
	"spec.memory": [128, 64, 32]
}
```

spec.cores 역색인에서 6을 찾을 수 있고, spec.memory 역색인에서 128을 찾을 수 있으니 문서는 최종 검색 결과에 포함된 것입니다. 이처럼 object 타입의 배열은 배열을 구성하는 객체 데이터를 서로 독립적인 데이터로 취급하지 않습니다.

> 어떤 경우에는 이들을 꼭 독립적인 데이터로 취급해야 할 필요가 있는데 이런 문제를 해결하기 위해 도입된 것이 nested 타입입니다.


##### nested 타입

nested 타입은 object 타입과는 다르게 배열 내 각 객체를 독립적으로 취급합니다. 위와 동일한 쿼리를 날리게 된다면 object 타입으로 선언했을 때와 다르게 아무것도 검색되지 않습니다.

nested 타입은 객체 배열의 각 객체를 내부적으로 별도의 루씬 문서로 분리해 저장합니다. 배열의 원소가 100개라면 부모 분서까지 해서 101개의 문서가 내부적으로 생성되는 것입니다. nested의 이런 동작 방식은 엘라스틱서치 내에서 굉장히 특수하기 때문에 nested 쿼리라는 전용 쿼리를 이용해서 검색해야 합니다.

nested 타입은 내부적으로 각 객체를 별도의 문서로 분리해서 저장하기 때문에 성능 문제가 있을 수 있습니다. 따라서 엘라스틱서치는 nested 타입의 무분별한 사용을 막기 위해 인덱스 설정으로 두 가지 제한을 걸어 놓았습니다.

- index.mapping.nested_fields.limit : 한 인덱스에 nested 타입을 몇 개까지 지정할 수 있는지 제한. 기본값 50
- index.mapping.nested_objects.limit : 한 문서가 nested 객체를 몇 개까지 가질 수 있는지를 제한. 기본값 10000

위 값들을 무리하게 높이면 OOM(Out Of Memory)의 위험이 있습니다.


| 타입 | object | nested |
| ---- | ------ | ------ |
| 용도     | 일반적인 계층 구조에 사용합니다.       | 배열 내 각 객체를 독립적으로 취급해야 하는 특수한 상황에서 사용합니다.       |
| 성능     | 상대적으로 가볍습니다.       | 상대적으로 무겁습니다. 내부적으로 숨겨진 문서를 생성합니다.       |
| 검색     | 일반적인 쿼리를 사용합니다.       | 전용 nested 쿼리로 감싸서 사용해야 합니다.       |


#### 3) 그 외 타입

엘라스틱서치의 필드 타입에는 여러 비즈니스 요구사항을 위한 타입들이 있습니다.

| 종류 | 설명 |
| ---- | ---- |
| geo_point | 위도와 경도를 저장하는 타입 |
| geo_shape | 지도상에 특정 지점이나 선, 도형 등을 표현하는 타입 |
| binary | base64로 인코딩된 문자열을 저장하는 타입 |
| long_range, date_range, ip_range 등 | 경곗값을 지정하는 방법 등을 통해 수, 날짜, IP 등의 범위를 지정하는 타입 |
| completion | 자동완성 검색을 위한 특수한 타입 |


#### 4) text 타입과 keyword 타입

문자열 자료형을 담는 필드에는 text 타입이나 keyword 타입 중 하나를 선택해 적용할 수 있습니다. text로 지정된 필드 값은 애널라이저가 적용된 후 색인됩니다. 들어온 문자열 값 그대로를 가지고 역색인을 구성하는 것이 아니라 값을 분석하여 여러 토큰으로 쪼갭니다. 이렇게 쪼개진 토큰으로 역색인을 구성합니다. 쪼개진 토큰에 지정한 필터를 적용하는 등의 후처리 작업 후 최종적으로 역색인에 들어가는 형태를 텀(term) 이라고 합니다.

반면에 keyword로 지정된 필드에 들어온 문자열 값은 여러 토큰으로 쪼개지 않고 역색인을 구성합니다. 애널라이저로 분석하는 대신 노멀라이저를 적용합니다. 노멀라이저는 간단한 전처리만을 거친 뒤 커다란 단일 텀으로 역색인을 구성합니다.

![[Pasted image 20240116222330.png]]


match 쿼리는 검색 대상 필드가 text 타입인 경우 검색 질의어도 애널라이저로 분석합니다.

```
GET mapping_test/_search
{
	"query": {
		"match": {
			"textString": "THE WORLD SAID HELLO"
		}
	}
}
```

위와 같은 검색을 진행하면 기본 standard 애널라이저가 질의어를 the, world, said, hello 로 4개의 텀으로 쪼갭니다.


정리하자면 text 타입은 애널라이저를 적용하여 여러 토큰으로 쪼개 색인하며, keyword 타입은 노멀라이저를 적용하여 단일 토큰으로 색인합니다.
색인 방식에 차이가 있어 text 타입은 주로 전문 검색에 적합하고 keyword 타입은 일치 검색에 적합합니다.

이 회에도 두 타입은 정렬과 집계, 스크립트 작업을 수행할 때 동작의 차이가 있습니다. 정렬과 집계, 스크립트 작업의 대상이 될 필드는 text 타입 보다는 keyword 타입을 쓰는 편이 좋습니다. keyword 타입은 기본적으로 doc_values 라는 캐시를 사용하고 text 타입은 fielddata 라는 캐시를 사용하기 때문입니다.


##### doc_values

- 엘라스틱서치의 검색은 역색인을 기반으로 한 색인을 사용합니다. 텀을 보고 역색인에서 문서를 찾는 방식입니다.
- 그러나 정렬, 집계, 스크립트 작업 시에는 접근법이 다릅니다. 문서를 보고 필드 내의 텀을 찾습니다.
- doc_values는 디스크를 기반으로 한 자료 구조로 파일 시스템 캐시를 통해 효율적으로 정렬, 집계, 스크립트 작업을 수행할 수 있게 설계되었습니다.
- 엘라스틱서치에서는 text와 annotated_text 타입을 제외한 거의 모든 필드 타입이 이 doc_values를 지원합니다.
- 또한 정렬, 집계, 스크립트 작업을 할 일이 없는 필드는 doc_values 를 끌 수 있습니다.
- doc_values를 지원하는 필드의 경우 기본값은 true 입니다.

```
PUT mapping_test/_mapping
{
	"properties": {
		"notForSort": {
			"type": "keyword",
			"doc_values": false
		}
	}
}
```



##### fielddata

- 대부분의 필드 타입은 doc_values를 사용할 수 있지만, text 타입은 파일 시스템 기반의 캐시인 doc_values를 사용할 수 없습니다.
- text 필드를 대상으로 정렬, 집계, 스크립트 작업을 수행할 때에는 fielddata라는 캐시를 이용합니다.
- fielddata를 사용한 정렬이나 집계 등의 작업 시에는 역색인 전체를 읽어들여 힙 메모리에 올립니다.
- fielddata의 이러한 동작 방식은 힙을 순식간에 차지해 Out Of Memory 등 많은 문제를 발생시킬 수 있습니다.
- fielddata의 기본값은 비활성화 상태입니다.

fielddata를 활성화하는 것은 매우 신중해야 합니다. text 필드의 역색인에는 애널라이저에 의해 분석된 텀이 들어갑니다. 즉, text 필드의 내용이 Hello, World! 라면 fielddata가 메모리에 올리는 역색인 텀은 이미 분석이 완료된 hello와 world라는 텀입니다.

> 이미 분석된 내용을 이용해서 집계 등을 수행하기 때문에 원하는 의도와 다른 결과가 나올 수 있으니 유의해야 합니다.

무엇보다 fielddata를 이용한 집계 등의 무거운 작업은 잦은 OOM을 발생시킬 수 있으며 클러스터 전체에 장애를 불러올 수도 있습니다.


- doc_values.      vs.      fielddata 간단 비교

|           | doc_values                                    | fielddata            |
| --------- | --------------------------------------------- | -------------------- |
| 적용 타입 | text와 annotated_text를 제외한 거의 모든 타입 | text, annotated_text |
| 동작 방식 | 디스크 기반이며 파일 시스템 캐시를 활용한다.                                              | 메모리에 역색인 내용 전체를 올린다. OOM에 유의해야 한다.                     |
| 기본값          | 기본적으로 활성화                                              | 기본적으로 비활성화                     |


> text 타입은 정렬, 집계 작업에 적합하비 않고 이유는 doc_values를 사용할 수 없기 때문입니다.


- text.      vs.      keyword 간단 비교

|  | text | keyword |
| ---- | ---- | ---- |
| 분석 | 애널라이저로 분석화여 여러 토큰으로 쪼개진 텀을 역색인에 넣는다. | 노멀라이저로 전처리한 단일 텀을 역색인에 넣는다. |
| 검색 | 전문 검색에 적합하다. | 단순 완전 일치 검색에 적합하다. |
| 정렬, 집계, <br>스크립트 | fielddata를 사용하므로 적합하지 않다. | doc_values를 사용하므로 적합하다. |


#### 5) \_source

\_source 필드는 문서 색인 시점에 엘라스틱서치에 전달된 원본 JSON 문서를 저장하는 메타데이터 필드입니다. 문서 조회 API나 검색 API가 클라이언트에게 반환할 문서를 확정하고 나면 이 \_source에 저장된 값을 읽어 클라이언트에게 반환합니다. \_source 필드 자체는 역색인을 생성하지 않기 때문에 검색 대상이 되지 않습니다.


- \_source 필드는 JSON 문서를 통째로 담기 때문에 디스크를 많이 사용합니다. \_source에 데이터를 저장하지 않도록 mappings 에 설정할 수도 있습니다.

```
PUT no_source_test
{
	"mappings": {
		"_source": {
			"enabled": false
		}
	}
}
```


그러나 이렇게 \_source를 비활성화하면 많은 문제가 생길 수 있습니다. 먼저 update_by_query API 를 이용할 수 없습니다.
reindex API 도 사용할 수 없습니다. reindex는 원본 인덱스에서 내용을 대상 인덱스에 새로 색인하는 작업입니다. 주로 매핑이나 샤드 개수 등 동적으로 변경하기 어려운 내용을 운영상의 이슈로 바꿔야만 할 때 인덱스를 통째로 새로 색인하거나 할 때 사용합니다.

> reindex는 엘라스틱서치 운영과 데이터 관리에 있어 핵심 API 입니다. reindex를 수행할 수 없다는 것만으로도 \_source 를 비활성화하지 않아야 할 충분한 이유가 됩니다.


##### 인덱스 코덱 변경

다른 성능을 희생하더라도 디스크 공간을 절약해야만 하는 상황이라면 \_source의 비활성화보다는 차라리 인덱스 데이터의 압축률을 높이는 편이 낫다고 공식 문서는 가이드하고 있습니다.

```
PUT codec_test
{
	"settings": {
		"index": {
			"codec": "best_compression"
		}
	}
}
```


##### synthetic source

엘라스틱서치 8.4 버전부터는 synthetic source라는 기능이 도입됐다. 해당 기능을 사용하는 인덱스는 \_source에 JSON 원문을 저장하지 않습니다. \_source 비활성화와 다른 점은 \_source를 읽어야 하는 때가 오면 문서 내 각 필드의 doc_values를 모아 \_source를 재조립해 동작한다는 점입니다.

즉, 인덱스의 모든 필드가 doc_values를 사용하는 필드여야 한다는 제약이 있으며 \_source를 읽어야 하는 작업의 성능은 떨어집니다. 대신 인덱스의 크기를 매우 많이 줄여줍니다.

```
PUT synthetic_source_test
{
	"mappings" {
		"_source": {
			"mode": "synthetic"
		}
	}
}
```


> \_source 와 관련한 설정 변경은 운영에 영향이 매우 크므로 특별한 경우에 한해 심사숙고하여 판단해야 합니다.



#### 6) index

index 속성은 해당 필드의 역색인을 만들 것인지를 지정합니다. 기본값은 true 입니다. false로 설정하면 해당 필드는 역색인이 없기 때문에 일반적인 검색 대상이 되지 않습니다.
문서의 내용 자체는 역색인 생성 여부와 상관없이 \_source라는 메타 필드에 저장됩니다. 또한 역색인을 생성하지 않는 것뿐이기 때문에 doc_values를 사용하는 타입의 필드라면 정렬이나 집계의 대상으로는 사용할 수 있습니다.

```
PUT mapping_test/_mapping
{
	"properties": {
		"notSearchableText": {
			"type": "text",
			"index": false
		},
		"docValuesSearchableText": {
			"type": "keyword",
			"index": false
		}
	}
}
```



#### 7) enabled

enabled 설정은 object 타입의 필드에만 적용됩니다. enabled가 false로 지정된 필드는 엘라스틱서치가 파싱조차 수행하지 않습니다. 데이터가 \_source에는 저장되지만 다른 어느 곳에도 저장되지 않습니다. 역색인을 생성하지 않기 대문에 검색도 불가능합니다. 정렬이나 집계의 대상도 될 수 없습니다.

파싱조차 수행하지 않기 때문에 이후 object 타입이 아닌 데이터가 들어와도 타입 충돌이 발생하지 않습니다. 마찬가지로 엘라스틱서치는 기본적으로 여러 타입이 혼용되는 배열을 허용하지 않지만 enabled가 false로 지정된 필드에는 아무런 체크를 수행하지 않기 때문에 타입이 혼용되는 배열도 데이터로 저장할 수 있습니다.

```
PUT mapping_test/_mapping
{
	"properties": {
		"notEnabled": {
			"type": "object",
			"enabled": false
		}
	}
}
```

이처럼 enabled를 false로 지정하면 역색인이나 doc_values를 생성하지 않고 파싱도 하지 않으므로 성능상의 이득을 꾀할 수 있습니다. 서비스 설계상 최종적으로 데이터를 \_source에서 확인만 하면 되고 그 외 어떠한 활용도 필요치 않은 필드가 있다면 enabled를 false로 지정하는 것을 고려해 볼 수 있습니다.



---


## 3. 애널라이저와 토크나이저

text 필드의 데이터는 애널라이저를 통해 분석돼 여러 텀으로 쪼개져 색인됩니다. 애널라이저는 0개 이상의 캐릭터 필터, 1개의 토크나이저, 0개 이상의 토큰 필터로 구성됩니다.

먼저 애널라이저는 입력한 텍스트에 캐릭터 필터를 적용하여 문자열을 변형시킨 뒤 토크나이저를 적용하여 여러 토큰으로 쪼갭니다. 쪼개진 토큰의 스트림에 토큰 필터를 적용해서 토큰에 특정한 변형을 가한 결과가 최종적으로 분석 완료된 텀입니다.

![[Pasted image 20240116225833.png]]


##### 1) analyze API

엘라스틱서치는 앞에서 설명한 애널라이저와 각 구성 요소의 동작을 쉽게 테스트해볼 수 잇는 analyze API를 제공하고 있습니다.

```
POST _analyze
{
	"analyzer": "standard",
	"text": "Hello, HELLO, World!"
}
```


#### 2) 캐릭터 필터

캐릭터 필터는 텍스트를 캐릭터의 스트림으로 받아서 특정한 문자를 추가, 변경, 삭제합니다. 애널라이저에는 0개 이상의 캐릭터 필터를 지정할 수 있습니다. 여러 캐릭터 필터가 지정됐다면 순서대로 수행됩니다. 엘라스틱서치에는 다음과 같은 내장 빌트인 캐릭터 필터가 있습니다.

- HTML strip 캐릭터 필터 : \<b> 와 같은 HTML 요소 안쪽의 데이터를 꺼냅니다.
- mapping 캐릭터 필터 : 치환할 대상이 되는 문자와 치환 문자를 맵 형태로 선언합니다.
- pattern replace 캐릭터 필터 : 정규 표현식을 이용해서 문자를 치환합니다.

```
POST _analyze
{
	"char_fillter": ["html_strip"],
	"text": "<p>I&apos;m so <b>happy</b>!</p>"
}
```



#### 3) 토크나이저

토크나이저는 캐릭터 스트림을 받아서 여러 토큰으로 쪼개어 토큰 스트림을 만듭니다. 애널라이저에는 한 개의 토크나이저만 지정할 수 있습니다.

- standard 토크나이저
	- 가장 기본적인 토크나이저입니다.
	- Unicode Text Segmentation 알고리즘을 사용하여 텍스트를 단어 단위로 나눕니다.
	- 대부분의 무장 부호가 사라집니다.
	- 필드 매핑에 특정 애널라이저를 지정하지 않으면 기본값으로 standard 애널라이저가 적용됩니다.
	- standard 애널라이저가 바로 이 standard 토크나이저를 이용합니다.

- keyword 토크나이저
	- 들어온 텍스트를 쪼개지 않고 그대로 내보냅니다.
	- 즉, 커다란 단일 토큰을 내보냅니다.
	- 토크나이저에서 특별한 동작을 수행하지 않기 때문에 쓸모없어 보일 수 있지만, 여러 캐릭터 필터, 토큰 필터와 함께 조합하면 다양한 커스텀 애널라이저 지정이 가능합니다.

- ngram 토크나이저
	- 텍스트를 min_gram 값 이상 max_gram 값 이하의 단위로 쪼갭니다. 예를 들어 min_gram 값이 2, max_gram 값을 3으로 지정한 뒤 hello 라는 텍스트를 토크나이저로 쪼개면 he, hel, el, ell, ll, llo 로 총 5개의 토큰으로 쪼개집니다.
	- 공백 문자나 문장 부호가 포함되어 사실상 활용 의미가 없는 토큰도 있습니다. 이런 문제를 피하기 위해 ngram 토크나이저에는 token_chars 라는 속성을 토앻 토큰에 포함시킬 타입의 문자를 지정할 수 있습니다.
	- token_chars를 특별히 지정하지 않을 때의 기본 동작은 모든 문자를 허용합니다.
	- 아래 표를 통해 ngram의 token_chars 속성으로 지정할 수 있는 값을 확인할 수 있습니다.
	- ngram 토크나이저는 엘라스틱서치에서 RDB의 LIKE \*검색어* 와 유사한 검색을 구현하고 싶을 때, 자동 완성 서비스를 구현하고 싶을 때 등에 주로 활용합니다.

| 종류 | 설명 |
| ---- | ---- |
| Letter | 언어의 글자로 분류되는 문자 |
| Digit | 숫자로 분류되는 문자 |
| whitespace | 띄어쓰기나 줄바꿈 문자 등 공백으로 인식되는 문자 |
| punctuation | ! 나 " 등 문장 부호 |
| symbol | $나 루트 와 같은 기호 |
| custom | custom_token_chars 설정을 통해 따로 지정한 커스텀 문자 |

- edge_ngram 토크나이저
	- edge_ngram 토크나이저는 ngram 토크나이저와 유사한 동작을 수행합니다. 먼저 입력된 텍스트를 token_chars에 지정되지 않은 문자를 기준으로 삼아 단어 단위로 쪼갭니다.
	- 그 다음 각 단어를 min_gram 값 이상 max_gram 값 이하의 문자 길이를 가진 토큰으로 쪼갭니다.
	- 하지만 ngram 토크나이저와는 다르게 생성된 모든 토큰의 시작 글자를 단어의 시작 글자로 고정시켜서 생성합니다.

- 그 외 토크나이저
	- letter 토크나이저 : 공백, 특수문자 등 언어의 글자로 분류되는 문자가 아닌 문자를 만났을 때 쪼갠다.
	- whitespace 토크나이저 : 공백 문자를 만났을 때 쪼갠다.
	- pattern 토크나이저 : 지정한 정규표현식을 단어의 구분자로 사용하여 쪼갠다.



#### 4) 토큰 필터

토큰 필터는 토큰 스트림을 받아서 토큰을 추가, 변경, 삭제합니다. 하나의 애널라이저에 토큰 필터를 0개 이상 지정할 수 있습니다. 토큰 필터가 여러 개 지정된 경우에는 순차적으로 적용됩니다. 내장 토큰 필터 중 일부는 아래와 같습니다.

- lowercase / uppercase 토큰 필터 : 토큰의 내용을 소문자/대문자로 만들어 줍니다.
- stop 토큰 필터 : 불용어를 지정하여 제거할 수 있습니다. the, a, an, in 등
- aynonym 토큰 필터 : 유의어 사전 파일을 지정하여 지정된 유의어를 치환합니다.
- pattern_replace 토큰 필터 : 정규식을 사용하여 토큰의 내용을 치환합니다.
- stemmer 토큰 필터 : 지원되는 몇몇 언어의 어간 추출을 수행합니다. 한국어는 지원하지 않습니다.
- trim 토큰 필터 : 토큰의 전후에 위치한 공백 문자를 제거합니다.
- truncate 토큰 필터 : 지정한 길이로 토큰을 자릅니다.



#### 5) 내장 애널라이저

애널라이저는 캐릭터 필터, 토크나이저, 토큰 필터를 조합하여 구성됩니다. 엘라스틱서치에는 내장 캐릭터 필터, 토크나이저, 토큰 필터를 조합하여 미리 만들어 놓은 다양한 내장 애널라이저가 있습니다.

- standard 애널라이저 : standard 토크나이저와 lowercase 토큰 필터로 구성됩니다. 특별히 애널라이저를 지정하지 않으면 적용되는 기본 애널라이저입니다.
- simple 애널라이저 : letter가 아닌 문자 단위로 토큰을 쪼갠 뒤 lowercase 토큰 필터를 적용합니다.
- whitespace 애널라이저 : whitespace 토크나이저로 구성됩니다.
- stop 애널라이저 : standard 애널라이저와 같은 내용이지만 뒤에 stop 토큰 필터를 적용해서 불용어를 제거합니다.
- keyword 애널라이저 : keyword 토크나이저로 구성됩니다. 특별히 분석을 실시하지 않고 하나의 큰 토큰을 그대로 반환합니다.
- pattern 애널라이저 : pattern 토크나이저와 lowercase 토큰 필터로 구성됩니다.
- language 애널라이저 : 여러 언어의 분석을 지원합니다. 한국어 분석은 지원하지 않습니다.
- fingerprint 애널라이저 : 중복 검출에 사용할 수 있는 특별한 핑거프린트용 토큰을 생성합니다.



#### 6) 애널라이저를 매핑에 적용

index.settings.analysis.analyzer 설정에 커스텀 애널라이저를 추가할 수 있습니다.


#### 7) 커스텀 애널라이저

내장 애널라이저로 목표하는 바를 달성할 수 없다면 커스텀 애널라이저 사용을 고려할 수 있습니다. 커스텀 애널라이저는 캐릭터 필터, 토크나이저, 토큰 필터를 원하는 대로 조합해 지정합니다.

```
PUT analyzer_test2
{
	"settings": {
		"analysis": {
			"char_filter": {
				"my_char_filter": {
					"type": "mapping",
					"mappings": [
						"i. => 1.",
						"ii. => 2.",
						"iii. => 3.",
						"iv. => 4."
					]
				}
			},
			"analyzer": {
				"my_analyzer": {
					"char_filter": [
						"my_char_filter"
					],
					"tokenizer": "whitespace",
					"filter": [
						"lowercase"
					]
				}
			}
		}
	},
	"mappings": {
		"properties": {
			"myText"; {
				"type": "text",
				"analyzer": "my_analyzer"
			}
		}
	}
}
```


#### 8) 플러그인 설치를 통한 애널라이저 추가와 한국어 형태소 분석

안타깝게도 한국어 형태소 분석을 지원하는 기본 내장 애널라이저는 없습니다. 하지만 엘라스틱서치가 공식 제공하는 nori 플러그인을 설치하면 한국어를 분석할 수 있습니다.

```
bin/elasticsearch-plugin install analysis-nori
```

플러그인을 설치할 때는 클러스터를 구성하고 있는 모든 노드에 설치해야 합니다. 클러스터를 구성하고 있는 모든 노드에서 각각 elasticsearch-plugin 명령을 수행하도록 합니다. 엘라스틱서치는 플러그인 설치를 완료하고 엘라스틱서치 클러스터를 재기동해야 새로 설치한 플러그인이 적용됩니다.



#### 9) 노멀라이저

노멀라이저는 애널라이저와 비슷한 역할을 하나 적용 대상이 text 타입이 아닌 keyword 타입의 필드라는 차이가 있습니다. 또한 애널라이저와는 다르게 단일 토큰을 생성합니다.

노멀라이저는 토크나이저 없이 캐릭터 필터, 토큰 필터로 구성됩니다. 또한 앞에서 살펴봤던 캐릭터 필터와 토큰 필터를 모두 조합할 수 있는 것은 아닙니다. 최종적으로 단일 토큰을 생성해야 하기 때문에 ASCII folding, lowercase, uppercase 등 글자 단위로 작업을 수행하는 필터만 사용할 수 있습니다.

엘라스틱서치가 제공하는 빌트인 노멀라이저는 lowercase 밖에 없습니다. 즉, 다른 방법으로 keyword 타입의 필드를 처리하려면 커스텀 노멀라이저를 조합해 사용해야 합니다.

```
PUT normalizer_test
{
	"settings": {
		"analysis": {
			"normalizer": {
				"my_normalizer": {
					"type": "custom",
					"char_filter": [],
					"filter": [
						"asciifolding",
						"uppercase"
					]
				}
			}
		}
	},
	"mappings": {
		"properties": {
			"myNormalizerKeyword": {
				"type": "keyword",
				"normalizer": "my_normalizer"
			},
			"lowercaseKeyword": {
				"type": "keyword",
				"normalizer": "lowercase"
			},
			"defaultKeyword": {
				"type": "keyword"
			}
		}
	}
}
```

위 설정에서 defaultKeyword 를 확인해보면 특별한 설정을 주지 않았습니다. 특별히 설정을 하지 않으면 standard 애널라이저를 기본 적용하는 text 타입과는 달리 keyword 타입에는 특별히 설저하지 않으면 아무런 노멀라이저도 적용하지 않습니다.



---


## 4. 템플릿

지금까지 인덱스 설정과 매핑, 그리고 매핑에 지정할 다양한 애널라이저 등을 알아봤습니다. 서비스와 데이터 설계에 따라 다를 수 있지만 엘라스틱서치를 실무에서 운영하다 보면 수시로 많은 양의 유사한 구조를 가진 인덱스를 생성해야 할 때가 많습니다. 템플릿을 사전에 정의해 두면 인덱스 생성 시 사전 정의한 설정대로 인덱스가 생성됩니다.


#### 1) 인덱스 템플릿

- index_patterns 부분에 인덱스 패턴을 지정합니다. 새로 생성되는 인덱스의 이름이 이 패턴에 부합하면 이 템플릿에 맞춰 인덱스가 생성됩니다.
- priority 값을 이용하면 여러 인덱스 템플릿간 우선 적용순위를 조정할 수 있습니다. priority 값이 높을수록 우선순위가 높습니다.

```
PUT _index_template/my_template
{
	"index_patterns": [
		"pattern_test_index-*",
		"another_pattern-*"
	],
	"priority": 1,
	"template": {
		"settings": {
			"number_of_shards": 2,
			"number_of_replicas": 2
		},
		"mappings": {
			"properties": {
				"myTextField": {
					"type": "text"
				}
			}
		}
	}
}
```


#### 2) 컴포넌트 템플릿

인덱스 템플릿을 많이 만들어 사용하다 보면 템플릿 간 중복되는 부분이 생깁니다. 이런 중복 부분을 재사용할 수 있는 작은 템플릿 블록으로 쪼갠 것이 컴포넌트 템플릿입니다. 컴포넌트 템플릿은 다양한 인덱스 템플릿을 관리할 때 효율적입니다.

```
PUT _component_template/timestamp_mappings
{
	"template": {
		"mappings": {
			"properties": {
				"timestamp": {
					"type": "date"
				}
			}
		}
	}
}


PUT _component_template/my_shard_settings
{
	"template": {
		"settings": {
			"number_of_shards": 2,
			"number_of_replicas": 2
		}
	}
}
```


이후에는 인덱스 템플릿을 생성할 때 재사용할 컴포넌트 템플릿 블록을 composed_of 항목에 넣으면 됩니다.

```
PUT _index_template/my_template2
{
	"index_patterns": ["timestamp_index-*],
	"composed_of": ["timestamp_mappings", "my_shard_settings"]
}
```



#### 3) 레거시 템플릿

인덱스 템플릿과 컴포넌트 템플릿 API는 엘라스틱서치 7.8.0 버전부터 추가된 기능입니다. 기존에 사용하던 템플릿 기능은 API에 \_index_template 대신에 \_template을 사용하였습니다.



#### 4) 동적 템플릿

동적 템플릿은 인덱스에 새로 들어온 필드의 매핑을 사전에 정의한대로 동적 생성하는 기능입니다.
동적 템플릿은 인덱스 템플릿과는 다르게 매핑 안에 정의합니다. 즉, 인덱스를 생성할 때나 인덱스 템플릿을 생성할 때 함께 지정합니다.

```
PUT _index_template/dynamic_mapping_template
{
	"index_patterns": ["dynamic_mapping*"],
	"priority": 1,
	"template": {
		"settings": {
			"number_of_shards": 2,
			"number_of_replicas": 2
		},
		"mappings": {
			"dynamic_templates": [
				{
					"my_text": {
						"match_mapping_type": "string",
						"match": "*_text",
						"mapping": {
							"type": "text"
						}
					}
				},
				{
					"my_keyword": {
						"match_mapping_type": "string",
						"match": "*_keyword",
						"mapping": {
							"type": "keyword"
						}
					}
				}
			]
		}
	}
}
```


위 동적 템플릿을 통해 새로운 필드가 들어올 때 그 데이터가 문자열 타입이라면 필드의 이름을 확인하고 \_text로 끝나면 text 타입, \_keyword로 끝나면 keyword 타입으로 지정하는 템플릿입니다.



#### 5) 빌트인 인덱스 템플릿

엘라스틱서치 7.9.0 이상 버전은 미리 정의된 빌트인 인덱스 템플릿을 제공합니다. 로그나 메트릭을 편리하게 수집하기 위한 X-Pack 전용 추가 기능 Elastic Agent에서 사용하기 위해 내장된 템플릿입니다.


---


## 5. 라우팅

라우팅은 엘라스틱서치가 인덱스를 구성하는 샤드 중 몇 번 샤드를 대상으로 작업을 수행할지 지정하기 위해 사용하는 값입니다. 라우팅 값은 문서를 색인할 때 문서마다 하나씩 지정할 수 있습니다. 작업 대상 샤드 번호는 지정된 라우팅 값을 해시한 후 주 샤드의 개수로 나머지 연산을 수행한 값이 됩니다.

라우팅 값을 지정하지 않고 문서를 색인하는 경우 라우팅 기본값은 \_id 값이 됩니다. 색인 시 라우팅 값을 지정했다면 조회, 업데이트, 삭제, 검색 등의 작업에서도 똑같이 라우팅을 지정해야 합니다.

아래 예시를 살펴보겠습니다.

```
PUT routing_test
{
	"settings": {
		"number_of_shards": 5,
		"number_of_replicas": 1
	}
}

PUT routing_test/_doc/1?routing=myid
{
	"login_id": "myid",
	"comment": "hello world",
	"created_at": "2020-09-08T22:14:09.123Z"
}
```

위 예시는 5개의 샤드를 가진 인덱스를 생성한 뒤 myid라는 값을 라우팅 값으로 지정하여 문서를 색인한 경우입니다.
엘라스틱서치는 검색할 때 라우팅 값을 기입하지 않으면 전체 샤드를 대상으로 검색을 수행하고, 라우팅 값을 명시하면 단일 샤드를 대상으로 검색합니다.

이후에 검색할 때 라우팅 값을 명시하여 검색을 한다면 엘라스틱서치가 라우팅 값을 이용하여 샤드 숫자를 정확히 특정한 뒤 그 단일 샤드를 대상으로 검색합니다. 즉, 라우팅 값을 다른 값으로 지정한다면 검색 결과에 원하는 문서가 포함되지 않을 수 있습니다. 문서가 저장된 샤드가 아닌 다른 샤드에 검색 요청이 들어가기 때문입니다.

운영 환경에서 문서를 색인하거나 검색할 때는 가능한 한 라우팅 값을 지정해 주는 것이 좋습니다. 지금은 복잡한 쿼리를 지정하지도 않았고 데이터의 양도 적기 때문에 큰 성능 차이를 느낄 수가 없습니다.
그러나 많은 데이터가 저장된 실제 운영 환경에서 검색을 수행한다면 라우팅 값을 지정했을 때와 그렇지 않을 때의 성능 차이는 매우 큽니다.

> 비즈니스 요건에 따라 라우팅 값을 지정할 수 없는 경우도 있지만 그러나 가능하면 라우팅 값을 지정할 수 있는 형태로 서비스와 데이터 설계를 하는 편이 효율적인 방법입니다.


#### 1) 인덱스 내에서의 \_id 고유성 보장

라우팅 값을 명시하지 않고 검색하면 전체 샤드를 대상으로 검색을 요청하게 됩니다. 이 경우에도 성능이 떨어지고 검색 결과도 달라질 수 있지만 검색 결과를 받아볼 수는 있습니다.

그러나 문서 조회 API를 통해 문서 단건을 조회할 때 라우팅 값을 명시하지 않는 경우에는 원하는 문서를 조회하지 못할 수도 있습니다. 문서 조회 API는 샤드 하나를 지정하여 조회를 수행합니다. 따라서 라우팅 값이 올바르게 명시되지 않는다면 엘라스틱서치는 이미 색인된 문서가 존재하는데도 다른 샤드에서 문서를 조회한 뒤 요청한 문서가 없다는 응답을 반환할 수 있습니다.

애초에 인덱스 내에서 \_id 값의 고유성 검증은 샤드 단위로 보장됩니다. 색인, 조회, 업데이트, 삭제 작업이 모두 라우팅 수행 이후의 단일 샤드 내에서 이뤄지기 때문입니다. 라우팅 값이 다르게 지정되면 한 인덱스 내에서 같은 \_id를 가진 문서가 여러 개 생길 수도 있습니다. 인덱스의 모든 샤드에서 \_id 값이 고유하도록 유지하는 것은 사용자의 책입입니다. 문서를 색인할 때부터 항상 라우팅 값을 올바르게 지정해야 합니다.



#### 2) 인덱스 매핑에서 라우팅을 필수로 지정하기

인덱스 매핑에서 \_routing 메타 필드를 지정하여 라우팅 값 명시를 필수로 설정할 수 있습니다.

```
PUT routing_test2
{
	"mappings": {
		"_routing": {
			"required": true
		}
	}
}
```


> 위 예시처럼 라우팅 값을 필수로 설정하면 라우팅 값이 명시되지 않은 색인, 조회, 업데이트, 삭제 요청은 실패하게 됩니다.

