
이번 장에서는 클러스터를 실무에서 관리하고 운영하는 데 필요한 내용을 학습합니다.

## 1. 클러스터 설정 API

엘라스틱서치를 운영하다 보면 서비스 중에 여러 설정을 동적으로 변경해야 할 때가 있습니다. 클러스터 설정 API는 그런 상황에서 클러스터와 관련된 설정을 확인 및 변경할 수 있습니다.

아래 요청을 통해 클러스터 설정을 확인할 수 있습니다.

```
GET _cluster/settings

{
	"persistent": {},
	"transient": {}
}
```

조회 결과를 보면 persistent와 transient 부분으로 나뉘어 있습니다. persistent 밑에 지정하는 설정은 클러스터를 풀 리스타트로 전체 재시작해도 유지됩니다. tranient 밑에 지정하는 설정은 클러스터를 전체 재시작하면 내용이 사라집니다. 그러므로 임시 설정을 지정하기에 좋습니다.

persistent와 transient를 꼭 동시에 설정할 필요는 없습니다. 이전에 지정했던 설정을 클러스터 설정 API의 본문에 다시 넣을 필요도 없습니다. 새로 추가하거나 변경하고 싶은 설정만 원하는 만큼 지정하면 됩니다.

```
PUT /_cluster/settings
{
	"persistent": {
		"indices.breaker.total.limit": "70%",
	},
	"transient": {
		"cluster.routing.allocation.enable": "primaries",
		"indices.breaker.total.limit": "90%"
	}
}
```

위와 같이 persistent와 transient에 동일한 설정이 지정되면 transient 설정이 우선 적용됩니다.

> 설정 적용 우선순위는 transient, persistent, config/elasticsearch.yml 파일 순입니다.

persistent 설정은 모든 마스터 후보 노드의 path.data 경로 내 파일로 저장됩니다. 클러스터 설정을 제거하려면 값을 null 로 지정하면 됩니다.


---

## 2. cat API를 통한 클러스터 관리와 모니터링

cat API는 엘라스틱서치의 여러 현재 상태를 조회할 수 있습니다. 대부분의 엘라스틱서치 REST API는 JSON 형식이 기본입니다. 하지만 cat API는 터미널에서 사람이 조회했을 때 보기 편한 형태로 응답하는 것이 목적입니다. 실제 운영 중 관리적인 목적이나 모니터링 용도로 cat API를 꾸준히 호출하게 될 것입니다.

```
GET _cat/health
GET _cat/indices
GET _cat/nodes
GET _cat/shards
GET _cat/segments
GET _cat/recovery
GET _cat/allocation
GET _cat/thread_pool
GET _cat/master
```

v 매개변수를 추가로 주면 응답에 각 칼럼의 제목이 포함됩니다. v 매개변수 외에도 아래와 같은 주요 공통 매개변수가 있습니다.

- v : 응답에 각 칼럼의 제목이 포함됩니다.
- h : 지정한 칼럼만 표시합니다.
- s : 칼럼을 지정해서 결과를 정렬합니다.
- help : 각 칼럼에 대한 설명을 보여줍니다.
- format : 출력 형식을 지정합니다. 값으로 text, json, yaml 등을 지정할 수 있습니다.


---

## 3. 인덱스 운영 전략

이번 절에서는 인덱스를 운영할 때 고려해야 할 중요한 요소와 운영 이슈에 대응하기 위한 기능 몇 가지를 알아봅니다.

#### 1) 템플릿과 명시적 매핑 활용

실제 서비스를 운영할 때는 매핑이 동적으로 생성되도록 하기보다는 최대한 명시적으로 매핑을 지정하는 것이 좋습니다. 하지만 서비스 특성상 추후 인입되는 데이터를 완전히 컨트롤할 수 없는 경우라면 사전에 알지 못한 상태에서 신규 필드가 추가될 수 있습니다. 이런 상황이 예상되는 서비스라면 인덱스 템플릿과 동적 템플릿을 최대한 활용하는 편이 좋습니다.

#### 2) 라우팅 활용

라우팅 지정은 성능을 유의미하게 상승시킵니다. 사전에 서비스 요건과 데이터 특성 등을 면밀히 파악하고 어떤 값을 라우팅으로 지정해야 가장 효율적일지를 설계해야 합니다. 모든 클라이언트가 이런 경우 라우팅 정책 내용을 숙지하면 좋을텐데 인덱스 매핑에서 \_routing을 true 로 설정해 라우팅 지정을 필수로 제한하는 방법도 검토가 필요합니다.

#### 3) 시계열 인덱스 이름

시계열 데이터를 색인한다면 인덱스 이름에도 시간값을 넣는 방법을 고려하면 좋습니다. 예를 들면 api-history-20210704나 access-log-202109와 같이 이름에 시간 표현이 들어간 인덱스를 주기적으로 생성하는 방법입니다. 데이터가 인입되면 데이터 내 특정 필드의 값이나 색인시의 시간을 기준으로 잡고 적합한 인덱스에 색인합니다.

이런 방법을 선택하면 오래된 데이터를 백업하고 삭제한는 것이 편합니다. 만약 하나의 인덱스에 데이터를 색인했다면 range 쿼리를 이용해서 데이터를 백업하고 delete by query로 삭제를 수행해야 합니다. 하지만 시계열 이름을 가진 인덱스라면 옛날 인덱스를 통째로 백업하고 삭제하면 그만입니다. 또한 이를 자동화할 수 있는 인덱스 생명주기 관리 기능과 함께 사용하기 좋습니다.

#### 4) alias

alias는 이미 존재하는 인덱스를 다른 이름으로 가리키도록 하는 기능입니다. 한 alias가 하나 이상의 인덱스를 가리키도록 지정할 수도 있습니다.

```
POST _aliases
{
	"actions": [
		{
			"add": {
				"index": "my_index",
				"alias": "my_alias_name"
			}
		},
		{
			"add": {
				"index": "my_index2",
				"alias": "my_alias_name",
				"is_write_index": true
			}
		}
	]
}
```

위와 같이 지정하고 my_alias_name을 대상으로 검색하면 my_index와 my_index2 둘 모두를 대상으로 검색합니다.

> 다만 여러 인덱스를 가리키는 alias는 단건 문서 조회 작업의 대상이 될 수 없습니다.

그렇다면 이 alias를 어디에 사용하는 것이 좋을까?

단순 로그성 데이터가 아니라 서비스에 직접 활용되는 데이터를 들고 있는 인덱스나, 요건 등이 변할 것이 예상되는 인덱스라면 모두 alias를 사전에 지정하는 것이 중요하다. 그리고 서비스에서는 실제 인덱스 이름이 아니라 이 alias를 가리키도록 설계해야 한다. 나중에 매핑이나 설정 등에 큰 변화가 필요할 때 새 인덱스를 미리 만들고 alias가 가리키는 인덱스만 변경하면 운영 중에 새 인덱스로 넘어갈 수 있다.

#### 5) 롤오버

앞서 alias를 학습할 때 하나의 alias에 여러 인덱스를 묶고 한 인덱스에서만 is_write_true를 지정하는 예를 살펴봤다. 이런 형태의 구성은 쓰기를 담당하는 인덱스 내 샤드의 크기가 너무 커지며서 새로운 인덱스를 생성해서 같은 alias 안에 묶은 뒤 is_write_true를 새 인덱스로 옮기는 방식으로 운영한다.

```
POST [롤오버 대상]/_rollover
```

여기서 롤오버 대상으로는 alias의 이름 또는 데이터 스트림의 이름이 들어갑니다.

> 롤오버를 수행할 alias 내 is_write_true 인덱스의 이름은 반드시 ^.\*-/d+$ 패턴을 따라야 합니다. 예를 들면, 인덱스 이름이 test-index-000001 인 경우 롤오버가 가능하지만 test-index 라면 롤오버가 수행되지 않습니다.

test-index-000001 인덱스가 is_write_true로 지정되어 있을 경우 롤오버를 수행하면 test-index-000002가 자동으로 생성되고 is_write_true가 넘어갑니다. 인덱스 이름의 숫자 부분은 반드시 여섯 자리일 필요는 없지만 롤오버가 자동으로 생성해 주는 이름은 숫자 부분을 여섯 자리로 맞춰서 생성합니다.

롤오버는 인덱스 생명 주기 관리 기능과 묶어 활용하기 좋습니다. 이 기능을 이용하면 인덱스가 생성된지 지정 시간 이상 지났거나 샤드의 크기가 지정 크기를 넘어섰거나 하는 등 다양한 지정 조건에 도달하면 엘라스틱서치가 자동으로 롤오버를 수행하도록 지정할 수 있습니다.

> 롤오버는 인덱스 생명 주기 관리 기능뿐만 아니라 데이터 스트림과 묶어서 사용하기도 좋습니다.


#### 6) 데이터 스트림

데이터 스트림은 내부적으로 여러 개의 인덱스로 구성되어 있습니다. 검색을 수행할 때는 해당 데이터 스트림에 포함된 모든 인덱스를 대상으로 검색을 수행하고, 문서를 추가 색인할 때는 가장 최근에 생성된 단일 인덱스에 새 문서가 들어갑니다. 롤오버 시에는 최근 생성된 인덱스의 이름 끝 숫자를 하나 올린 새 인덱스가 생성됩니다. 즉, 데이터 스트림은 마치 여러 인덱스를 묶고 is_write_true인덱스를 하나 둔 alias와 유사하게 동작합니다.

![[Pasted image 20240308143803.png]]

이런 구조의 alias를 인덱스 템플릿과 연계해서 조금 더 시계열 데이터 사용 패턴에 맞게 정형화하고 간단하게 사용할 수 있도록 정제한 것이 데이터 스트림이라고 보면 됩니다.

데이터 스트림과 alias의 차이점은 다음과 같습니다. 먼저 데이터 스트림을 구성하는 인덱스는 <span style="color:#ff0000">뒷받침 인덱스</span>라고 부르며 모두 hidden 속성입니다. 이 뒷받침 인덱스는 이름 패턴이 고정이고 롤오버 시 명시적인 새 인덱스 이름 지정이 불가능합니다. 그리고 반드시 인덱스 템플릿과 연계해서 생성해야 합니다. 문서 추가는 가능하지만 업데이트 작업은 불가능하고 반드시 @timespamp 필드가 포함된 문서만을 취급합니다.

데이터 스트림을 만들기 위해 먼저 ILM(Index Lifecycle Management) 정책을 생성합니다. ILM과의 연동 자체가 필수는 아니나 데이터 스트림 기능 자체가 ILM과의 연계를 염두에 두고 개발된 기능입니다.

```
PUT _ilm/policy/test-ilm-policy
{
	"policy": {
		"phases": {
			"hot": {
				"min_age": "0ms",
				"actions": {
					"rollover": {
						"max_primary_shard_size": "4gb",
						"max_age": "1d"
					}
				}
			},
			"delete": {
				"min_age": "15d",
				"actions": {
					"delete": {}
				}
			}
		}
	}
}
```

위 설정을 보면 데이터 스트림 내에 생성된 뒷받침 인덱스의 샤드 크기가 4GB를 넘어서면 자동 롤오버를 수행합니다. 또한 이와는 별개로 하루가 지나도 자동 롤오버를 수행합니다. 15일이 지난 뒷받침 인덱스는 자동 삭제합니다.

> 데이터 스트림은 반드시 인덱스 템플릿과 연계해야 합니다. 책 271p 부터 관련 내용은 확인 가능합니다.

검색 API 사용 시 인덱스 이름이 들어갔던 자리에 데이터 스트림의 이름을 넣으면 됩니다. 검색 외에도 대부분의 API에서 인덱스 이름 대신 데이터 스트림 이름을 넣으면 됩니다.

데이터 스트림은 특정한 시스템의 모니터링용 지표 데이터를 수집하기 위한 용도 등 그냥 문제 시간대의 데이터를 버려도 큰 문제가 되지 않는 경우에 사용하기 좋습니다. 데이터와  서비스의 특징을 잘 파악하고 적절한 전략을 세우도록 합니다.

#### 7) reindex

reindex는 원본 인덱스 내 문서의 \_source를 읽어서 대상 인덱스에 새로 색인하는 작업입니다.

```
POST _reindex
{
	"source": {
		"index": "source_index"
	},
	"dest": {
		"index": "target_index"
	},
	"conflicts": "abort"
}
```

만약 작업 도중 버전 충돌이 일어나면 작업은 해당 부분까지만 진행되고 취소된다. 이를 원치 않으면 conflicts 값을 proceed로 지정해야 합니다. 이 경우 충돌이 발생한 문서는 건너뛰고 다음 작업을 진행한다. 기본값은 abort다.

reindex는 작업 특성상 매핑에서 \_source가 활성화되어 있어야 합니다. \_source를 비활성화하면 reindex도 업데이트 작업도 수행할 수 없습니다. 운영 리스크를 고려하면 \_source를 비활성화하는 일은 없어야 합니다.

검색 쿼리를 지정해서 검색에 걸린 일부 문서만을 재인덱싱할 수도 있습니다.

```
POST _reindex
{
	"source": {
		"index": "source_index",
		"query": {
			// ...
		}
	},
	"dest": {
		"index": "target_index"
	}
}
```


reindex는 주로 관리적인 목적으로 호출하기 때문에 작업의 크기도 큰 경우가 많습니다. reindex 요청을 보낸 뒤 작업이 끝날 때까지 대기하는 것은 효츌적이지 않습니다. reindex도 update_by_query, delete_by_query 처럼 wait_for_completion=false를 지정해서 작업을 tasks에 등록하고 비동기적 실행을 할 수 있습니다.

#### 8) shrink로 샤드 개수 줄이기

shrink는 샤드의 개수를 줄이면서 인덱스를 새로 생성하는 작업입니다. 원본이 될 현재 인덱스 이름과 새로 생성할 인덱스의 이름을 지정하고 요청 본문에 샤드 개수를 지정합니다.

```
POST [현재 인덱스 이름]/_shrink/[새로 생성할 인덱스 이름]
PUT [현재 인덱스 이름]/_shrink/[새로 생성할 인덱스 이름]
{
	"settings": {
		"index.number_of_replicas": 2,
		"index.number_of_shards": 1
	}
}
```

어차피 인덱스를 새로 만드는 것이니 겉으로 보기에는 reindex와 차이가 없는 기능으로 보입니다. 그렇다면 shrink와 reindex는 무엇이 다를까??

reindex는 기본적으로 새 인덱스에 문서를 다시 새로 색인하는 과정입니다. 새 인덱스를 생성할 때 설정이나 매핑을 원하는 대로 바꿔서 생성한 뒤 새 인덱스 설정에 맞춰 색인이 수행됩니다. shrink는 기존 인덱스의 세그먼트를 새 인덱스로 하드링크합니다. 샤드 개수를 제외한 인덱스 설정과 매핑이 유지됩니다.

shrink는 하드 링크를 해야 하기 때문에 reindex에 비해 제약이 많습니다. 예를 들어 새로 생성할 인덱스의 샤드 개수는 원본 인덱스 샤드 개수의 약수여야 합니다.

실무에서 관리적인 목적으로 shrink를 직접 사용할 일은 많지 않습니다. 운영 중인 인덱스의 샤드 개수를 줄여야만 한다면 굳이 shrink가 가지고 있는 여러 제약사항을 감수할 필요 없이 안전하게 reindex를 수행하는 편이 좋습니다.

#### 9) split으로 샤드 개수 늘이기

split은 샤드의 개수를 늘리면서 인덱스를 새로 생성하는 작업입니다. 다음과 같이 원본이 될 현재 인덱스 이름과 새로 생성할 인덱스의 이름을 지정하고 요청 본문에 샤드 개수를 지정합니다.

```
POST [현재 인덱스 이름]/_split/[새로 생성할 인덱스 이름]
PUT [현재 인덱스 이름]/_split/[새로 생성할 인덱스 이름]
{
	"settings": {
		"index.number_of_shards": 4
	}
}
```

split 역시 shrink와 마찬가지로 새 인덱스 생성 후에 원본 인덱스의 세그먼트를 하드 링크하는 방식으로 작업이 진행됩니다. index.number_of_routing_shards 설정은 라우팅 과정에서 라우팅 값을 몇 덩어리로 쪼갤지를 지정합니다. 이 값의 기본값은 index.number_of_shards와 같습니다. index.number_of_routing_shards 값은 index.number_of_shards 값의 배수로 지정해야 합니다.

다만 split 역시 읽기 전용 인덱스에만 작업할 수 있다는 제약 때문에 운영 도중 사용할 일이 많지 않습니다. alias와 reindex를 조합해 사용하는 경우가 더 많습니다.

#### 10) 다중 필드

실무에서는 서비스가 이미 운영 중이고 매핑도 확정되었는데 특정 필드를 기존과 다른 방법으로 색인해야 하는 상황이 생깁니다. 예를 들면, keyword 타입으로 색인 중이던 데이터를 애널라이저를 적용한 text 타입으로 색인해야 할 수 있습니다. 이런 상황에서 사용할 수 있는 것이 다중 필드입니다.

다중 필드는  필드 하나에 여러 이름을 붙인 뒤 각각 다른 매핑을 적용해서 사용할 수 있게 해줍니다. 아래 예시를 살펴보겠습니다.

먼저 keyword 타입을 가진 인덱스를 하나 생성합니다.

```
PUT multi-fields-test
{
	"mappings": {
		"properties": {
			"my_string_field": {
				"type": "keyword"
			}
		}
	}
}
```

변경되는 서비스 요건에서는 이 my_string_field 필드를 여러 애널라이저를 적용한 text 타입으로도 활용하고자 합니다. 다음과 같이 매핑을 지정해 봅니다.

```
PUT multi-fields-test/_mapping
{
	"properties": {
		"my_string_field": {
			"type": "keyword",
			"fields": {
				"text_standard": {
					"type": "text",
					"analyzer": "standard"
				},
				"text_whitespace": {
					"type": "text",
					"analyzer": "whitespace"
				}
			}
		}
	}
}
```

fields 라는 속성을 지정한 뒤 그 밑에 text_standard, text_whitespace 라는 필드 이름을 추가로 지정했습니다. 그리고 그 밑에 각 이름별로 적용될 매핑 설정을 지정합니다. 이렇게 다중 필드를 지정하면 이후 이 필드는 my_string_field 이름으로 접근할 때는 keyword 타입으로, my_string_field.text_standard 이름으로 접근할 때는 standard 애널라이저를 적용한 text 타입으로 동작합니다.

인덱스에 한 번 지정된 매핑은 변경할 수 없습니다. 하지만 새 매핑을 추가하는 것은 가능하기에 운영 도중 다중 필드를 추가할 수 있습니다. 다중 필드 추가는 서비스 조건 변경 이슈에 대응할 수 있는 좋은 수단입니다. 다만 다중 필드 매핑이 추가된 이후 들어오는 데이터부터 적용됩니다. 기존 데이터는 다시 색인을 해야 추가된 다중 필드가 적용됩니다.

> 또한 색인이 추가되는 만큼 색인 성능이 감속하는 점, 디스크와 메모리를 조금 더 사용하게 되는 점을 염두에 두여야 합니다. 이 부분이 문제가 된다면 결국 reindex를 고려해야 합니다.


#### 11) 타입이 계속 변경되는 데이터

클라이언트를 완전히 통제할 수 없는 상황에서 서비스 운영을 하다 보면 같은 필드 이름으로 어떤 때는 long 값이, 어떤 때는 string 값이, 어떤 때는 object 값이 들어오는 일이 생깁니다. 엘라스틱서치는 특성상 그런 형태의 사용에 적합하지 않으며 이런 데이터는 색인되지 않는다는 사실을 클라이언트에게 주지시켜야 합니다.

> 하지만 반드시 이렇게 이용해야 하는 필드가 있다면 매핑에서 타입을 object로 지정한 뒤 enabled: false를 지정하는 방법을 사용할 수 있습니다. 이후 타입 충돌로 인한 문서 색인 거부는 발생하지 않겠지만 해당 필드에 대한 검색은 포기해야 합니다.

#### 12) 루씬 텀 길이 제약

keyword 타입이나 text 타입 등 string 필드에 매우 긴 데이터가 들어오면 루씬 텀 길이 제약에 걸려 문서의 색인이 거부되는 경우가 생길 수 있습니다. 만약 keyword 타입을 사용하고 이런 긴 데이터를 색인에 넣어둬야 할 특별한 이유가 없다면 처음부터 매핑에 ignore_above를 지정하는 것을 고려할 수 있습니다. ignore_above는 지정한 수치보다 긴 텀을 역색인에 넣지 않도록 해 주는 속성입니다.

```
PUT [인덱스 이름]
{
	"mappings": {
		"properties": {
			"my_string_field": {
				"type": "keyword",
				"ignore_above": 1000
			}
		}
	}
}
```

> ignore above에 지정하는 값은 바이트 단위가 아니라 문자 단위다.

#### 13) 대량 색인이 필요할 때

서비스 출시를 위한 초기 데이터 세팅이나 마이그레이션, reindex를 통한 인덱스 재생성 등 대량 색인이 필요할 때가 있습니다. 이런 경우에는 색인 도중 데이터를 조회할 필요가 없다. 따라서 다음과 같은 설정을 통해 색인 속도를 높일 수 있습니다.

```
PUT my_index/_settings
{
	"refresh_interval": "-1",
	"number_of_replicas": 0
}
```

refresh를 끄고 복제본 샤드 개수를 0으로 지정해서 복제본 생성을 중지하는 설정이다. 작업이 끝나면 원래 설정으로 복구하는 것을 잊지 말아야 한다.


---

## 4. 샤드 운영 전략

인덱스의 샤드 개수는 한 번 지정하면 reindex 등의 특별한 작업을 수행하지 않는 한 변경할 수 없다. 그런데 샤드 개수를 어떻게 지정하느냐에 따라 엘라스틱서치 클러스터 전체의 성능이 크게 달라진다.

#### 1) 샤드의 크기와 개수 조정

클러스터에 샤드 숫자가 너무 많아지면 클러스터 성능이 눈에 띄게 떨어진다. 어떤 기준으로 어떻게 밸런스를 잡아야 하는 것일까

먼저 조금 더 중요한 원칙은 샤드 하나의 크기를 일정 기준 이하로 유지해야 한다는 것입니다. 전체 샤드의 수를 체크하는 것은 그다음입니다. 

> 샤드의 크기를 기준 크기 이하로 유지하는 선에서 전체 샤드 개수를 최대한 줄이는 방향으로 접근해야 합니다.

엘라스틱 블로그에 올라온 가이드를 보면 샤드 하나당 20 ~ 40GB 크기가 적절하다고 소개한다. 그러나 실제 운영 경험상으로는 샤드 하나당 크기가 20GB만 되어도 다양한 상황에서 꽤 느리고 무겁다. 20 ~ 40 GB 샤드도 어느 정도 버틸 만은 하지만 수 GB 내외 수준에서 조정하는 것이 좋다.

또한 위 가이드에서는 힙 1GB당 20개 이하의 샤드를 들고있는 것이 적절하다고 설명합니다.

> 힙 1GB당 샤드 20개 기준은 빡빡하기는 하나 이 기준을 지키는 것을 어느 정도 이상적인 목표로 삼으며 밸런스를 조정하면 좋다.


#### 2) 모든 노드가 충분히 일을 하고 있는지

엘라스틱서치의 샤드는 이름 자체가 의미하듯 샤딩, 즉 분산처리를 위해 생긴 개념이다. 전체 샤드 개수를 줄이는 것만 생각하다 보면 분산처리의 강점을 충분히 살리지 못할 수도 있다. 특정 인덱스의 주 샤드와 복제본 샤드가 모든 노드에 고르게 퍼지도록 설정하는 것도 중요한 요소다.

#### 3) 미래에 데이터가 커질 것을 고려

운영상의 이슈로 인해 미래에 커질 수 있는 데이터 사이즈를 어느 정도 고려해서 설정해야 합니다.

인덱스 이름에 시간값을 넣어 정기적으로 신규 인덱스를 생성하는 성질의 데이터는 템플릿에 설정된 number_of_shards 값을 봐 가면서 변경하면 이런 문제를 어느 정도 유연하게 대처할 수 있습니다.

그러나 단일 인덱스에 서비스 데이터를 담고 있다면 그렇게 하기는 어렵습니다. 이런 경우라면 미래에 커질 데이터 사이즈를 넉넉히 예상해서 설정해 두는 것이 좋습니다. 그렇더라도 버틸 수 없는 상태까지 오면 결국 reindex를 할 수밖에 없습니다. reindex를 위해서 미리 alias를 설정해 두는 것을 잊지 않아야 합니다.

#### 4) 테스트 수행

실제로는 인덱스에 어떤 성격의 데이터가 얼마나 있는지에 따라 샤드 양상이 매우 다릅니다. 여러 통제된 조건에서 테스트를 해 보고 결정하는 것이 가장 좋습니다. 특히 성능에 민감한 인덱스라면 테스트가 더 중요합니다. 가능하다면 실제 서비스 조건과 유사한 조건으로 테스트를 수행해 보고 그 결과를 참고해 개수를 지정하도록 하는게 좋습니다.


---

## 5. 롤링 리스타트

엘라스틱서치 운영 중에는 롤링 리스타트를 수행할 일이 매우 많습니다. 동적으로 변경할 수  없는 설정의 적용, 플러그인 설치나 삭제의 적용, 엘라스틱서치의 버전 업그레이드 등 다양한 상황에서 롤링 리스타트가 필요합니다. 그리고 무엇보다 장애 상황에서 문제를 일으키고 있는 노드를 재기동하기 위해 많이 수행됩니다.

롤링 리스타트는 크게 샤드 할당 비활성화, flush 수행, 노드 재기동, 샤드 할당 활성화, green 상태까지 대기 순으로 수행됩니다.

#### 샤드 할당 비활성화

노드를 재기동하기 위해 엘라스틱서치 프로세스를 종료시키면 클러스터 구성에서 노드가 빠집니다. 빠진 노드가 데이터 노드라면 주 샤드를 새로 지정하고 줄어든 복제본 개수를 맞추기 위해 복제본 샤드를 새로 할당해 생성하는 작업이 수행됩니다. 사실 롤링 리스타트 과정에서는 복제본 샤드를 새로 생성하는 작업이 필요없습니다. 재기동한 노드가 클러스터에 합류하면 자신이 들고 있던 샤드를 복제본 샤드로 다시 띄우기 때문입니다. 하지만 클러스터 입장에서는 노드가 재기동을 위해 빠졌는지 장애로 인해 빠졌는지 구분할 방법이 없습니다. 그저 노드가 빠지면 일정 시간 대기한 뒤 복제본 샤드 할당 작업을 수행할 뿐입니다.

불필요한 작업을 막기 위해 엘라스틱 클러스터의 샤드 할당 작업을 제어할 수 있습니다.

```
PUT _cluster/settings
{
	"transient": {
		"cluster.routing.allocation.enable": "primaries"
	}
}
```

#### flush 수행

flush를 수행해서 translog를 비우고 데이터를 디스크에 안전하게 기록합니다. 루씬 인덱스에 반영되지 않고 아직 translog에 남아 있는 내용은 노드 재기동시 샤드 복구 과정에서 처리되기 때문에 이 작업은 반드시 필요한 작업은 아닙니다. 하지만 이런 작업에는 시간과 자원이 소요되므로 미리 flush를 수행하고 재기동을 하는 것이 좋습니다.

```
POST _flush
```

#### 노드 재기동

샤드 할당을 비활성화하고 flush를 무사히 완료했으면 노드를 재기동합니다. 프로세스를 kill 하고 띄우면 됩니다. 프로세스를 새로 띄우기 전에 반드시 기존 프로세스가 완전히 종료됐는지 확인하고 띄워야 합니다.

이후 노드가 완전히 기동하고 클러스터에 합류할 때까지 대기합니다. cat nodes API를 호출해서 재기동한 노드가 클러스터에 잘 붙었는지 확인합니다.

클러스터의 상태가 좋지 않거나 장애 상황에서 재기동을 수행 중이라면 cat nodes API의 수행에 오래 시간이 걸릴 수 있습니다.

#### 샤드 할당 활성화

노드 재기동에 성공했으면 샤드 할당을 다시 활성화합니다.

```
PUT _cluster/settings
{
	"transient": {
		"cluster.routing.allocation.enable": "all"
	}
}
```

#### green 상태까지 대기

이후 클러스터 상태가 green이 될 때까지 대기합니다.

```
GET _cat/health?v
```

green 상태가 되면 다시 샤드 할당을 비활성화하고 다음 노드를 재기동하는 작업을 필요한 만큼 반복합니다.


---

## 6. 스냅샷과 복구

데이터 백업은 실제 서비스를 운영할 때 빼놓을 수 없는 중요한 이슈입니다. 스냅샷은 동작 중인 엘라스틱서치 클러스터의 인덱스를 백업하는 방법입니다. 인덱스를 원하는 다양한 스토리지에 스냅샷으로 찍어 두면 필요할 때 복구할 수 있습니다.

엘라스틱서치는 데이터를 파일 기반으로 저장합니다. 그러므로 엘라스틱서치의 데이터 디렉터리를 그대로 복제해 두면 백업이 된다고 생각할 수 있습니다. 그러나 엘라스틱서치는 파일 시스템 레벨의 복제본으로부터 데이터를 복구하는 방법을 공식적으로 지원하지 않습니다. 파일 시스템 레벨의 단순 복사는 가장 마지막 발버둥에 가깝습니다.

동작 중인 엘라스틱서치의 데이터를 백업하고 복구하는 데에는 공식 기능인 스냅샷을 사용하는 것이 좋습니다.

#### 1) 스냅샷 저장소 등록과 설정

스냅샷을 찍으려면 먼저 스냅샷 저장소를 등록해야 합니다. 기본적으로 지원되는 것은 nfs와 같은 공유 파일 시스템입니다. 그 외 다른 타입의 저장소는 별도의 플러그인을 설치해야 합니다. 엘라스틱서치는 공식으로 HDFS, S3, Azure, Google 클라우드 스토리지 타입 플러그인을 제공합니다.

엘라스틱서치의 스냅샷은 증분 백업 방식으로 동작합니다. 이를 위해 스냅샷 작업을 시작할 때 저장소 내 다른 모든 스냅샷의 정보를 메모리로 올리는 작업을 선행합니다. 따라서 저장소에 스냅샷을 많이 가진 저장소는 느려집니다.

#### 2) 스냅샷을 생성하고 조회하기

스냅샷 저장소를 생성했으니 이제 스냅샷을 찍어 데이터를 백업해 보자. 요청 방법은 다음과 같습니다.

```
PUT _snapshot/[저장소 이름]/[스냅샷 이름]?wait_for_completion=false
{
	"indices": "my-index-202108*, my-index-202109*"
}
```

요청 본문의 indices 부분에는 데이터를 백업할 인덱스를 지정합니다. 쉼표나 와일드 카드 문자를 이용해 여러 인덱스를 대상으로 지정할 수 있습니다. wait_for_completion 매개변수를 지정해서 작업을 비동기로 요청할 수 있습니다.

그러나 update_by_query나 reindex 등에서 wait_for_completion 매개변수를 사용해 비동기 요청할 때와 달리 스냅샷의 비동기 요청은 tasks에 작업을 등록하지 않습니다.

```
GET _snapshot/[저장소 이름]/[스냅샷 이름]
```

응답의 state 부분을 확인하면 작업이 끝났는지 여부를 알 수 있습니다. 작업이 아직 진행 중이라면 IN_PROGRESS 상태입니다. 스냅샷이 어떤 인덱스를 포함하고 있는지 확인할 수 있는데 . 으로 시작하는 인덱스가 포함됩니다. 이는 전역 상태와 feature를 살펴봐야 합니다.

- 전역 상태와 feature
	- 스냅샷을 찍을 때 요청 본문에 include_global_state 라는 설정을 지정할 수 있습니다.
	- 이는 현재 persistent 클러스터 설정이나 내부 시스템 인덱스의 내용 등 전역 상태를 스냅샷에 저장할 것인지 지정합니다.
	- 이 설정의 기본값은 true 입니다.

```
GET _snapshot/[저장소 이름]/[스냅샷 이름]/_status
```

위 명령어를 이용해 스냅샷이 몇 개의 파일을 포함하고 있는지, 크기는 얼마나 되는지, 증분된 파일은 몇 개인지 등 더 상세한 정보를 얻을 수 있습니다.

#### 3) 스냅샷에서 인덱스 복구하기

스냅샷 백업으로부터 인덱스를 복구할 수 있습니다.

```
POST _snapshot/[저장소 이름]/[스냅샷 이름]/_restore
{
	"indices": "some-index-2021080*",
	"include_global_state": false,
	"feature_states": ["kibana"]
}
```

요청 본문의 indices에는 스냅샷에 포함된 인덱스 중 복구를 원하는 인덱스를 지정합니다. indices를 지정하지 않으면 전체 인덱스를 복구합니다. include_global_state는 전역 상태를 복구할지 여부를 지정합니다. 기본값은 false 입니다. 이 값이 true인 경우 기존 persistent 클러스터 설정, 레거시가 아닌 인덱스 템플릿, 인제스트 파이프라인, 인덱스 생명 주기 정책을 완전히 삭제한 뒤 스냅샷에 저장된 정보로 대체합니다. feature의 시스템 인덱스도 현재 데이터를 완전히 덮어쓰는 방식으로 복구됩니다.

feature_states 에는 스냅샷의 feature_states에 저장된 부분 중 복구를 원하는 feature 목록을 지정합니다. 역시 마찬가지로 feature_states를 통해서 복구되는 시스템 인덱스나 설정값은 현재 데이터를 완전히 덮어쓰므로 유의해야 합니다.

#### 4) 스냅샷 삭제하기

- 한 저장소에 너무 많은 스냅샷이 쌓이면 성능이 저하됩니다.
- 주기적으로 스냅샷을 삭제하며 관리를 해야 합니다.

```
DELETE _snapshot/[저장소 이름]/[스냅샷 이름]
```

엘라스틱서치의 스냅샷은 증분 백업입니다. 따라서 스냅샷이 삭제될 때는 그 스냅샷에 포함된 파일이 다른 스냅샷에 사용되는지를 파악하고 다른 스냅샷에 영향을 주지 않는 파일만 삭제됩니다.

#### 5) 스냅샷 생명 주기 관리

스냅샷 생명 주기 관리는 지정한 시각에 지정한 내용의 스냅샷을 찍고 지정한 기간보다 오래된 스냅샷은 삭제하는 등의 작업을 자동으로 수행하는 정책을 등록해 스냅샷을 관리하는 기능입니다. 키바나의 UI로 직관적으로 정책을 등록 및 삭제할 수 있습니다.


---

## 7. 명시적으로 세그먼트 병합하기

세그먼트 병합을 수행한 이후에는 검색 성능의 향상, 디스크나 메모리 등의 자원 절약을 기대할 수 있습니다. 다음과 같이 forcemerge API를 통해 명시적으로 세그먼트 병합을 수행할 수 있습니다.

```
POST [인덱스 이름]/_forcemerge?max_num_segments=1
```

max_num_segments에는 최대 몇 개의 세그먼트로 병합을 수행할지 지정합니다. 관리적인 목적으로 수행할 때는 대부분 1로 지정하면 됩니다. 명시적인 세그먼트 병합은 더 이상 추가 데이터가 색인되지 않는다는 것이 보장될 때 수행해야 합니다. 그리고 인덱스에 더 이상 변화가 없다면 단일 세그먼트를 유지하는 것이 가장 좋습니다.

> 세그먼트 병합은 비용이 큰 작업입니다. 그러므로 매일 한산한 시간대에 forcemerge를 수행하는 배치 작업을 걸어두는 등의 방법을 선택하는 것이 좋습니다.


---

## 8. 샤드 할당 필터링과 데이터 티어 구조

클러스터를 대규모로 운영한다면 하드웨어적 특성이나 서버의 물리적인 위치 이슈 등으로 인해 클러스터 내 샤드 할당 배분을 제어하고 싶을 때가 있습니다. 이번 절에서는 노드에 원하는 속성을 지정하고 그 속성을 통해 샤드를 특정 노드에 할당하는 방법을 알아보겠습니다.

#### 1) 노드 속성과 샤드 할당 의식

클러스터를 A 특성을 가진 노드 부분 집합과 B 특성을 가진 노드 부분 집합 등으로 구분한 뒤, 한 샤드의 주 샤드와 복제본 샤드를 서로 다른 부분 집합에 배분할 수 있습니다. 엘라스틱서치를 기동시킬 때 노드에 원하는 커스텀 속성을 지정한 뒤 이를 기반으로 샤드 할당을 제어하면 됩니다.

elasticsearch.yml 에 다음과 같이 지정합니다.

```yaml
node.attr.my_rack_id: rack_one
```

속성 이름은 자유롭게 지을 수 있습니다. 노드 기동 시 속성을 지정했다고 바로 샤드 할당에 이 속성이 활용되는 것이 아닙니다. 샤드 할당 시 이러한 속성을 고려하라고 클러스터에 알려줘야 합니다.

```
PUT _cluster/settings
{
	"persistent": {
		"cluster.routing.allocation.awareness.attributes": "my_rack_id,my_other_id"
	}
}
```

my_rack_id가 rack_onde인 노드 부분 집합과 rack_two인 노드 부분 집합으로 나눠 클러스터를 기동하면 엘라스틱서치는 동일한 샤드에 대해 주 샤드와 복제본 샤드를 서로 다른 부분 집합에 배분합니다. 다만 이렇게 우선 할당하는 것이 강제적 규칙은 아닙니다.

#### 2) 클러스터 단위 샤드 할당 필터링

샤드 할당 의식 속성과 함께 적용할 수 있는 규칙 중 하나로 클러스터 단위 할당 필터링이 있습니다.

```
PUT _cluster/settings
{
	"persistent": {
		"cluster.routing.allocation.include.my_rack_id": "rack_one,rack_two"
	}
}
```

위의 예시는 my_rack_id 노드 속성으로 rack_one과 rack_two 중 적어도 하나 이상을 가진 노드에 샤드를 할당하는 규칙을 지정하는 예시입니다. include 외에 require과 exclude를 쓸 수 있습니다.

- include : 지정한 속성들 중 적어도 하나 이상을 노드 속성으로 가진 노드에 샤드를 할당합니다.
- require : 지정한 속성들을 모두 노드 속성으로 가진 노드에 샤드를 할당합니다.
- exclude : 지정한 속성들 중 하나라도 노드 속성으로 가진 노드에는 샤드를 할당하지 않습니다.

클러스터 단위 샤드 할당 필터링을 사용하는 가장 전형적인 예는 특정 노드를 클러스터에서 아예 제거할 때입니다. 클러스터 단위 샤드 할당 필터링을 사용하면 노드를 제거하기 전에 샤드 이동을 먼저 수행할 수 있으므로 조금이지만 더 안정적으로 노드를 제거할 수 있습니다.

```
PUT _cluster/settings
{
	"persistent": {
		"cluster.routing.alloation.exclude._ip": "10.0.0.1"
	}
}
```

> 여기서는 내장 속성을 이용했습니다. 내장 속성으로는 name, \_host_ip, \_publish_ip, \_ip, \_host, \_id 등이 있습니다.

#### 3) 인덱스 단위 샤드 할당 필터링

샤드 할당 의식 속성, 클러스터 단위 샤드 할당 필터링과 함께 인덱스 단위 샤드 할당 필터링을 적용할 수 있습니다. 특정 인덱스에 샤드 할당 필터링 설정을 지정하는 방식으로 사용합니다.

```
PUT my_index/_settings
{
	"index.routing.allocation.include.my_rack_id": "rack_one,rack_two",
	"index.routing.allocation.require.my_other_id": "attr_one"
}
```

include, require, exclude로 속성을 지정하는 점, 커스텀 속성과 내장 속성을 모두 사용할 수 있는 점 등 모두 클러스터 단위 샤드 할당 필터링과 동일합니다. 인덱스 템플릿이나 인덱스 생명 주기 관리 정책과 연동해 관리하기 좋기 때문에 클러스터  단위 샤드 할당 필터링보다는 사용처가 넓습니다. 바로 이어 얘기할 데이터 티어 구조와 함께 사용하기 좋습니다.

#### 4) 데이터 티어 구조

데이터 티어 구조는 데이터 노드를 용도 및 성능별로 hot-warm-cold-frozen 티어로 구분해 클러스터를 구성하는 방법입니다. 노드의 역할로 data를 지정하지 않고, data_content, data_hot, data_warm, data_cold,  data_frozen을 지정해 클러스터를 구성합니다.

성능 차이가 많이 나는 장비를 가지고 클러스터를 구성해야 하거나 최근 데이터 위주로 사용하는 방식의 시계열 데이터를 운영할 때 채택하기 좋습니다.

인덱스를 원하는 데이터 티어에 할당하려면 index.routing.allocation.include.\_tier_preference 설정을 해당 데이터 티어의 이름으로 지정하면 됩니다.

```
PUT my_index/_settings
{
	"index.routing.allocation.include._tier_preference": "data_warm,data_hot"
}
```

위와 같이 지정하면 먼저 data_warm 노드에 인덱스를 할당합니다. 만약 data_warm 노드에 인덱스를 할당할 수 없으면 data_hot 노드에 인덱스를 할당합니다. 이 값을 명시적으로 null 로 지정하면 샤드 할당에 있어 데이터 티어를 고려하지 않습니다. 인덱스 생성 시 기본값은 data_content이고 데이터 스트림 내 인덱스 생성 시 기본값은 data_hot 입니다.

직접 명시적으로 이 값을 수정해서 데이터 티어를 옮기는 것보다는 주로 인덱스 템플릿이나 인덱스 생명 주기 관리 정책과 함께 사용해서 자동으로 인덱스를 관리하는 방법으로 사용하는 편입니다. 
이어서 인덱스 생명 주기 관리를 알아보겠습니다.


---

## 9. 인덱스 생명 주기 관리

인덱스 생명 주기 관리(ILM)은 인덱스를 hot-warm-cold-frozen-delete 페이즈로 구분해서 지정한 기간이 지나면 인덱스를 다음 페이즈로 전환시키고 이때 지정한 작업을 수행하도록 하는 기능입니다. 인덱스와 데이터 스트림을 매우 편하게 관리할 수 있게 해줍니다.

- hot : 현재 업데이트가 수행되고 있고 읽기 작업도 가장 많은 상태
- warm : 인덱스에 더 이상 업데이트가 수행되지는 않지만 읽기 작업은 들어오는 상태
- cold : 인덱스에 더 이상 업데이트가 수행되지 않고 읽기 작업도 가끔씩만 들어오는 상태, 검색은 되어야 하나 속도가 느려도 괜찮은 상황
- frozen : 인덱스에 더 이상 업데이트가 수행되지 않고 읽기 작업도 거의 들어오지 않는 상태, 검색은 되어야 하나 속도가 상당히 느려도 괜찮은 상황
- delete : 인덱스가 더 이상 필요없고 삭제되어도 무방한 상태

이 가운데 frozen 페이즈에서는 운영 중인 인덱스를 검색 가능한 스냅샷으로 변환하는 작업이 가능하지만 엔터프라이즈 등급의 구독에서만 사용 가능합니다.

#### 페이즈 전환

페이즈 전환에는 시간 조건을 사용하면 됩니다. 인덱스가 생성된 이후 얼마만큼의 시간이 지났는지, 그리고 현재 페이즈에서 수행해야 하는 액션을 모두 끝냈는지 체크해서 조건을 모두 만족하면 다음 페이즈로 전환합니다. 

> 이때, 인덱스에 미할당 샤드가 남아 있어 yellow 상태이더라도 다음 페이즈로 전환하므로 이 점을 참고해야 합니다.

#### 페이즈 액션 수행

ILM 은 indices.lifecycle.poll_interval에 지정된 주기로 인덱스의 상태를 확인하고 필요한 작업을 수행합니다. 기본값은 10분입니다. 각 페이즈에 지정된 액션의 수행 순서는 ILM이 제어합니다.

#### 페이즈 액션 종류
 
 지정할 수 있는 페이즈 액션의 종류는 다음과 같습니다.

- 롤오버
- 읽기 전용으로 만들기
- 세그먼트 병합
- shrink
- 인덱스 우선순위 변경
- 할당
- migrate
- 동결
- 스냅샷 대기
- 삭제

#### ILM 정책 생성과 적용

ILM 정책은 키바나의 UI로 쉽게 생성 및 관리할 수 있습니다. stack management 메뉴에서 index lifecycle policies 하위 메뉴로 들어간 후 create policy 버튼을 눌러 정책을 생성할 수 있습니다.

ILM 정책을 인덱스에 적용하려면, 인덱스의 index.lifecycle.name 설정에서 ILM 정책의 이름을 지정해 주면 됩니다.

```
PUT [인덱스 이름]/_settings
{
	"index.lifecycle.name": "my-policy-test"
}
```


---

## 10. 서킷 브레이커

엘라스틱서치는 과도한 요청이 들어왔을 때 이를 최대한 처리하다 죽도록 하는 정책이 아니라 처음부터 과도한 요청은 거부하는 정책을 채택했습니다. 노드가 중단되는 사태를 방지하는 것이 더 중요하다고 판단한 것입니다. 서킷 브레이커는 문제를 발생시킬 만한 무거운 작업의 수행을 사전 차단합니다. 요청이 어느 정도의 메모리를 사용할지 예상하는 방법과 실제 현재 노드가 사용 중인 메모리를 체크하는 방법을 사용합니다.

서킷 브레이커로 요청이 거부되는 상황에 대한 처리 책임은 기본적으로 클라이언트에게 있습니다. 서킷 브레이커 발동으로 요청이 거부됐다고 해서 엘라스틱서치가 이를 자동으로 재시도해 주지는 않습니다.

- 필드 데이터 서킷 브레이커
- 요청 서킷 브레이커
- 실행 중 요청 서킷 브레이커
- 부모 서킷 브레이커
- 스크립트 컴파일 서킷 브레이커

서킷 브레이커 설정은 클러스터 설정 API를 이용해 동적으로 설정할 수 있습니다.

```
PUT _cluster/settings
{
	"transient": {
		"indices": {
			"breaker": {
				"fielddata.limit": "30%",
				"request.limit": "70%",
				"total.limit": "90%"
			}
		},
		"network": {
			"breaker": {
				"inflight_requests.limit": "95%"
			}
		}
	}
}
```

> 특히 실제 메모리를 체크하는 부모 서킷 브레이커의 효과가 좋습니다.

#### 스크립트 컴파일 서킷 브레이커

스크립트 컴파일 서킷 브레이커는 특정 시간 내에 수행할 수 있는 인라인 스크립트 컴파일의 수를 제한합니다. 기본값은 75/5m으로, 매 5분 이내에 75번의 스크립트 컴파일까지 허용한다는 뜻입니다. 이 서킷 브레이커는 다른 서킷 브레이커와 다르게 메모리 기반이 아닙니다.

```
PUT _cluster/settings
{
	"transient": {
		"script": {
			"max_compilations_rate": "50/5m"
		}
	}
}
```


---

## 11. 슬로우 로그 설정

검색이나 색인 작업 시 너무 오랜 시간이 소요되면 별도로 로그를 남기도록 설정할 수 있습니다. 이러한 슬로우 로그는 장애 원인을 추적하는 데 매우 큰 도움이 됩니다. 개본적으로 어떤 설정도 지정돼 있지 않으므로 직접 설정해야 합니다. 동적으로 변경할 수 있는 설정입니다.

```
PUT _settings
{
	"index.search.slowlog": {
		"threshold": {
			"query.warn": "10s",
			"query.info": "5s",
			"query.debug": "2s",
			"query.trace": "500ms",
			"fetch.warn": "1s",
			"fetch.info": "800ms",
			"fetch.debug": "500ms",
			"fetch.trace": "200ms"
		}
	}
}
```

> 이 로그는 샤드 레벨에서 측정됩니다. 검색 요청 전체에 소요된 시간을 측정해 로깅하는 것이 아닙니다.

느린 검색 로그를 남기도록 설정하면 로그 디렉터리에 별도로 \[클러스터 이름]\_index_search_showlog.log 파일이 생깁니다.

![[Pasted image 20240311011145.png]]

위 예시처럼 어떤 인덱스, 어떤 샤드에서 어떤 쿼리가 얼마나 시간을 소요했는지 확인할 수 있습니다.


---

## 12. 버전 업그레이드

엘라스틱서치는 다른 오픈소스 프로젝트나 개방형 프로젝트에 비해 버전이 상당히 빨리 올라가는 편입니다. 새 버전에 추가되는 신기능이 필요 없는 상황이라 하더라도 특별한 사정이 없다면 어느 정도 버전 업그레이드를 염두에 둬 가며 운영하는 것이 좋습니다.

#### 목표 버전과 업그레이드 방법 선택

업그레이드 방법에는 <span style="color:#ff0000">롱링 업그레이드</span>와 <span style="color:#ff0000">풀 리스타트</span> 업그레이드가 있습니다. 롤링 업그레이드는 노드를 한 대씩 롤링 리스타트하는 과정에서 종료한 노드의 버전을 올려서 재기동하며 한 대씩 버전을 올리는 방법입니다. 풀 리스타트 업그레이드는 엘라스틱서치 클러스터를 완전히 종료하고 전체 노드의 버전을 업데이트한 뒤 새로 일괄 기동하여 버전을 올리는 방법입니다.

> 롤링 업그레이드가 가능한 조건이면 풀 리스타트 업그레이드보다는 롤링 업그레이드를 선택하는 것이 좋다.

같은 메이저 버전끼리는 롤링 업그레이드를 할 수 있습니다. 그 외에는 5.6에서 6.8로, 6.8에서 7 버전대로, 7.17에서 8 버전대로 롤링 업그레이드를 할 수 있습니다.

#### 업그레이드 어시스턴트로 점검

키바나의 업그레이드 어시스턴트는 클러스터 상황을 점검하고 업그레이드를 수행하기 전에 해결할 사항을 알려줍니다.
stack management 메뉴에서 upgrade assistant 라는 하위 메뉴를 통해 사용할 수 있습니다.

#### 인덱스 생성 버전 확인

메이저 버전이 올라가면 엘라스틱서치 인덱스 데이터의 내부 구조도 바뀝니다. 엘라스틱서치는 직전 메이저 버전에서 생성한 인덱스까지는 하위 호환을 보장해 주지만 메이저 버전이 두 개 이상 차이나는 이전 버전에서 생성한 인덱스는 인식하지 않습니다. 따라서 메이저 버전을 둘 이상 올리는 경우에서는 차근차근 중간 버전을 한 단계씩 밟아 가야 합니다. 메이저 버전을 하나 올리고 전체 인덱스를 reindex하고 다시 메이저 버전을 올리는 작업 순으로 반복해야 합니다.

#### 데이터 백업

백업에는 엘라스틱서치의 공식 스냅샷 기능을 사용하는 것도 좋습니다. 공식 복원 기능을 사용할 수도 있으니 백업과 복원 작업 자체에 품이 많이 들지 않는 것이 강점입니다. 그러나 약점도 있습니다. 스냅샷도 메이저 버전이 둘 이상 차이나는 클러스터에서는 복원이 불가능합니다.

백업 작업에 많은 노력을 투자할 수 있는 상황이라면 엘라스틱서치가 아닌 별도 스토리지에 주요 데이터의 \_source 내용을 백업하는 것이 가장 안전합니다.

#### 플러그인 호환 점검

엘라스틱서치 플러그인은 기본적으로 특정 엘라스틱서치 버전에 맞게 동작하도록 작성되었습니다. 플러그인의 구조 설계 자체가 엘라스틱서치 버전에 종속적입니다. 즉, 엘라스틱서치 버전을 변경하면 기존 플러그인을 그대로 사용할 수 없습니다.

#### 롤링 업그레이드

롤링 업그레이드를 수행할 때는 마스터 후보 노드가 아닌 노드부터 업그레이드하는 것이 좋습니다. 일반적으로 상위 버전의 노드는 하위 버전의 마스터 후보 노드에 붙을 수 있지만, 반대의 경우는 그렇지 않을 수 있습니다. 마스터 후보가 아닌 노드를 모두 업그레이드한 후에 마스터 후보 노드를 한 대 씩 업그레이드 합니다.

노드 한 대가 무사히 업그레이드되어 클러스터에 잘 붙으면 다음 노드를 대상으로 같은 작업을 반복합니다. 버전 업그레이드 작업은 실행할 바이너리를 교체하는 작업입니다. 새 엘라스틱서치 파일을 받아 압축을 풀고 심볼릭 링크를 교체합니다.

#### 엘라스틱 스택 업그레이드

엘라스틱서치 외에 키바나, 로그스태시 등 엘라스틱이 제공하는 엘라스틱 스택의 주요 구성요소를 함께 사용하고 있다면 이들도 업그레이드의 영향을 받습니다. 여유가 있다면 영향성을 검토한 뒤 함께 업그레이드하는 것이 좋습니다. 엘라스틱서치 공식 홈페이지에서 추천하는 업그레이드 순서는 엘라스틱서치 - 하둡, 엘라스틱서치, 키바나, 자바 고수준 REST 클라이언트, 로그스태시, 비츠, APM 서버, 엘라스틱 에이전트 순입니다.