
이번 장에서는 인덱스에 실제로 문서를 색인, 조회, 업데이트, 삭제하는 방법을 알아봅니다. 또한 다양한 검색 쿼리와 페이지네이션, 집계에 대해서도 학습합니다. 즉, 이번 절의 내용을 학습하고 나면 엘라스틱서치를 이용해 실제 비즈니스 문제를 해결하는 구체적인 방법을 습득할 것입니다.


---


## 1. 단건 문서 API

#### 1) 색인 API

```
PUT [인덱스 이름]/_doc/[_id값]
POST [인덱스 이름]/_doc
PUT [인덱스 이름]/_create/[_id값]
POST [인덱스 이름]/_create/[_id값]
```

기본이 되는 API는 PUT \[인덱스 이름]/\_doc/\_id값 입니다. 만약 해당 인덱스에 이미 같은 \_id 값을 가진 문서가 있다면 새 문서로 덮어씌웁니다.

POST 메서드는 \_id 값을 지정하지 않고 색인을 요청할 경우에 사용합니다. \_id 값을 지정하지 않으면 엘라스틱서치가 \_id 값을 랜덤하게 지정하고, 이 경우 항상 새로운 문서가 생성됩니다.


##### \[라우팅]
색인 시 routing 매개변수로 라우팅 값을 지정할 수 있습니다. 데이터 설계상 가능하다면 라우팅 값을 설정해 주는 것이 좋습니다.

```
PUT routing_test/_doc/2?routing=myid2
{
	"login_id": "myid2",
	"comment": "hello elasticsearch",
	"created_at": "2020-12-01T00:00:12.3728"
}
```


##### \[refresh]
색인 시 refresh 매개변수를 지정하면 문서를 색인한 직후에 해당 샤드를 refresh 해서 즉시 검색 가능하게 만들 것인지 여부를 지정할 수 있습니다. refresh 매개변수에는 다음과 같은 3개 옵션을 지정할 수 있습니다.

| refresh 값 | 동작 방식 |
| ---------- | --------- |
| true       | 색인 직후 문서가 색인된 샤드를 refresh 하고 응답을 반환합니다.          |
| wait_for           | 색인 이후 문서가 refresh될 때까지 기다린 후 응답을 반환합니다. true로 지정했을 때와는 다르게 refresh를 직접 유발하지는 않는다. 다만 너무 많은 요청이 refresh 대기 중인 경우 강제로 refresh가 수행될 수 있습니다.          |
| false           | 아무 값도 지정하지 않았을 때 기본값입니다. refresh와 관련된 동작을 수행하지 않습니다.          |

문서 조회 API를 사용하면 문서가 색인된 직후에 조회해도 색인된 내용을 조회할 수 있습니다. 그러나 검색 API를 사용할 때는 다릅니다.

> 검색의 경우 검색은 refresh 단계까지 와야 새로 색인된 내용의 검색이 가능해지기 때문입니다.

실제 서비스를 만들다 보면 색인 직후 검색 API를 사용하는 경우가 있어 이런 경우 refresh 값을 고려할 수 있습니다.

그러나 true 나 wait_for 를 지정해서 색인하기 전에 반드시 성능에 대한 고려가 필요합니다. refresh는 비용이 큰 작업입니다. refresh 값을 true로 지정해 호출하면 성능이 크게 저하될 수 있고 이러한 사용 방식은 너무 많은 작은 세그먼트를 생성하기 때문에 이후 검색 성능과 세그먼트 병합 과정의 부담도 커지게 됩니다.

이런 문제를 피하기 위해 각 요청의 응답 시간을 조금 희생하기로 결정하고 refresh를 wait_for로 지정하는 방법이 있습니다. wait_for를 사용하면 index.refresh_interval 값의 크기만큼 응답 시간이 지연될 수 있습니다. index.max_refresh_listeners 설정값 이상의 요청이 refresh를 대기하는 중이라면 강제로 refresh가 수행됩니다. 즉, 기본 설정값 기준으로는 초당 1천 개 이상의 색인 요청이 wait_for로 들어오는 경우 강제 refresh가 수행됩니다.

> 기본적으로는 문서 색인 요청의 결과가 검색 역색인에 즉시 동기적으로 반영되어야 하는 요청은 많지 않도록 서비스를 설계하는 것이 바람직합니다.

> 색인 요청한 문서의 내용을 동기적으로 즉시 확인해야 하는 경우 최대한 검색 API를 사용하지 않고 조회 API를 활용하도록 설계하는 것이 좋다. 그리고 대량의 색인이 필요한 경우 단건 색인 API가 아니라 복수 문서를 다루는 bulk API를 사용해야 합니다.



#### 2) 조회 API

조회 API는 문서 단건을 조회합니다. 조회 API는 검색과는 다르게 색인이 refresh 되지 않은 상태에서도 변경된 내용을 확인할 수 있습니다. 애초에 고유한 식별자를 지정해서 단건 문서를 조회하는 것은 역색인을 사용할 필요가 없습니다. 그리고 translog 에서도 데이터를 읽어올 수 있기 때문입니다. 조회 API는 인덱스 이름과 \_id 값을 명시하고 가운데에 \_doc 이나 \_source 를 지정해 호출합니다.

```
GET [인덱스 이름]/_doc/[_id값]
GET [인덱스 이름]/_source/[_id값]
```

기본적인 조회에슨 \_doc 을 이용하면 됩니다. \_doc 을 이용하면 인덱스, \_id 를 포함한 기본적인 메타 데이터를 함께 조회할 수 있습니다.

반면 이런 메타데이터를 필요로 하지 않고 문서의 본문만을 원한다면 \_source 를 지정하여 호출할 수 있습니다.

> \_source 를 지정한 검색 결과는 메타데이터 없이 문서 본문만 조회됩니다.


##### \[필드 필터링]
조회 API 사용 시 \_source_includes 와 \_source_excludes 옵션을 사용하면 결과에 원하는 필드만 필터링해 포함시킬 수 있습니다. \_source_includes 를 지정하면 지정한 필드만을 결과에 포함합니다.

\_source_excludes 옵션은 \_source_includes 적용까지 끝낸 최종 결과에서 제외할 필드를 선택하는 옵션입니다.


##### \[라우팅]
조회 API도 색인 API와 마찬가지로 라우팅을 반드시 제대로 지정해야 합니다. 색인했을 때 사용한 라우팅 값과 동일한 라우팅 값을 사용해 조회해야 의도하는 문서를 조회할 수 있습니다.


#### 3) 업데이트 API

업데이트 API는 지정한 문서 하나를 업데이트 합니다. 다음과 같이 POST 메서드를 사용하고 \_doc 대신 \_update 를 사용하면 됩니다. 그리고 요청 본문에 doc 이나 script를 지정하여 업데이트할 내용을 기술합니다. 업데이트 API는 기본적으로 부분 업데이트로 동작합니다.

> 문서 전체를 교체하려면 색인 API를 사용하면 됩니다.

```
POST [인덱스 이름]/_update/[_id값]
```


##### \[doc에 내용을 직접 기술하여 업데이트]
- detect_noop
	- 업데이트 API를 호출하면 엘라스틱서치는 그 작업을 수행하기 전에 업데이트 내용이 기존 문서 내용을 실질적으로 변경하는지 여부를 확인합니다.
	- 엘라스틱서치는 업데이트 작업 수행 전 요청이 이런 noop 요청인지를 확인하고 만약 noop 요청이라면 쓰기 작업을 수행하지 않습니다.
	- noop을 검사하는 것은 불필요한 디스크 I/O를 줄일 수 있습니다.
	- 일반적인 상황에서는 detect_noop을 활성화하는 것이 좋습니다. detect_noop은 기본적으로 활성화 되어 있습니다.

- doc_as_upsert
	- 업데이트 API는 기본적으로 기존 문서의 내용을 먼저 읽어들인 뒤 업데이트를 수행합니다. 즉, 기존 문서가 이미 없다면 요청은 실패합니다.
	- 기존 문서가 없을 때에는 새로 문서를 추가하는 upsert 기능이 필요하다면 doc_as_upsert 옵션을 true로 지정해 주면 됩니다.
	- 기본값은 false 입니다.


##### \[script를 이용하여 업데이트]

별도의 스크립트 언어를 넣어서 문서를 업데이트 하는 방법이 있습니다. 현재는 엘라스틱서치의 자체 스크립트 언어인 painless를 이용합니다. painless는 엘라스틱서치의 스크립팅을 위해 개발된 언어입니다.

```
POST update_test/_upate/1
{
	"script": {
		"source": "ctx._source.views += params.amount",
		"lang": "painless",
		"params": {
			"amount": 1
		}
	},
	"scripted_upsert": false
}
```

아래는 스크립트를 사용한 업데이트시 접근할 수 있는 문맥 정보 목록입니다.

![[Pasted image 20240130212034.png]]


##### \[라우팅과 refresh]
업데이트 API에도 색인 API와 마찬가지로 routing과 refresh 옵션을 지정할 수 있습니다. 문서를 색인했을 때 사용한 값과 동일한 값으로 라우팅을 지정해 업데이트해야 의도한 대로 동작합니다. refresh 옵션 또한 조회 API와 동일하게 동작합니다.

> refresh 옵션을 true 나 wait_for 로 지정할 때는 반드시 성능을 고려해야 합니다.


#### 4) 삭제 APi

삭제 API는 지정한 문서 하나를 삭제합니다. 다음과 같이 DELETE 메서드를 사용하여 호출합니다.

```
DELETE [인덱스 이름]/_doc/[_id값]
```

일반적으로 한 번 삭제한 문서는 되돌릴 수 없기 때문에 삭제 작업은 항상 신중해야 합니다. 또한 뒤 부분을 빠뜨리고 DELETE \[인덱스 이름] 으로 호출을 날릴 경우 인덱스 전체가 삭제되기 때문에 실수하지 않도록 조심해야 합니다.


---

## 2. 복수 문서 API

대형 서비스에서 발생하는 데이터를 한 건 단위로 HTTP에 실어 나르면 오버헤드가 너무 큽니다. 실제 서비스 환경에서는 최대한 단건 문서 API 보다는 복수 문서 API를 활용해야 합니다. REST API 한 번에 많은 데이터를 담아 보내야 HTTP 통신에 드는 비용이 절약됩니다.

#### 1) bulk API

대량의 데이터를 색인할 때 많이 사용됩니다. bulk API는 엘라스틱서치의 다른 API와는 다르게 요청 본문을 JSON이 아니라 NDJSON 형태로 만들어서 보냅니다. NDJSON은 여러 줄의 JSON을 줄바꿈 문자로 구분하여 요청을 보내는 형태입니다.

```
POST _bulk
{"index": {"_index": "bulk_test", "_id": "1"}}
{"field1": "value1"}
{"delete": {"_index": "bulk_test", "_id": "2"}}
{"create": {"_index": "bulk_test", "_id": "3"}}
{"field1": "value3"}
{"update": {"_id": "1", "_index": "buld_test"}}
{"doc": {"field2": "value2"}}
{"index": {"_index": "bulk_test", "_id": "4", "routing": "a"}}
{"field1": "value4"}
```

요청의 종류로는 index, create, update, delete가 사용됩니다. index와 create는 색인 요청입니다. create는 새 문서를 생성하는 것만 허용하고 기존 문서를 덮어쓰지 않습니다. index는 기존에 동일한 \_id로 문서가 존재하는지 여부와 상관없이 항상 색인 작업을 수행합니다.

전체 응답 내 각 세부 응답은 요청의 순서와 동일합니다. 각 세부 요청의 결과 상태 코드를 status 필드로 응답합니다.

> 전체 응답의 상태 코드는 200입니다. 하지만 각각의 응답 상세 코드는 404일 수도 있기 때문에 200을 받았다고 안심하지 말고 세부 응답을 파싱해서 원하는 대로 작업이 수행됐는지 확인해야 합니다.

##### \[bulk API의 작업 순서]
bulk API에 기술된 작업은 반드시 그 순서대로 수행된다는 보장이 없습니다. 여러 개의 주 샤드에 넘어간 각 요청은 각자 독자적으로 수행합니다. 따라서 요청 간 순서는 보장되지 않습니다.

그러나 완전히 동일한 인덱스, \_id, 라우팅 조합을 가진 요청은 반드시 동일한 주 샤드로 넘어갑니다. 따라서 한 bulk API 내에서 이 조합이 같은 요청, 즉 동일한 문서에 대한 요청은 bulk API에 기술된 순서대로 동작합니다.

##### \[bulk API의 성능]
네트워크를 통해 요청을 여러 번 반복해서 호출해야 한다면 이를 묶어 한꺼번에 전송하는 것이 일반적으로 성능상 이득입니다. bulk API 요청 한 번에 몇 개의 요청을 모아서 보내는 것이 성능상 적절한지는 정해져 있지 않습니다. 각 요청의 크기나 데이터 특성 등에 따라 달라지기 때문입니다. 그러므로 성능 이슈가 있는 경우 수치를 조절해 가며 실험해 보는 것이 좋습니다. 또한 HTTP 요청을 청크로 보내는 것은 성능을 떨어지게 만들기 때문에 피해야 합니다.


#### 2. multi get API

multi get API는 \_id를 여럿 지정하여 해당 문서를 한 번에 조회하는 API 입니다. 단건 조회 API를 반복해서 사용하는 것보다 성능이 좋습니다.

```
GET _mget
GET [인덱스 이름]/_mget
```

> 응답은 요청했던 순서대로 문서의 내용을 모아 단일 응답으로 돌아옵니다.

만약 \_mget 앞에 인덱스 이름을 명시했다면 bulk API 때와 마찬가지로 이 인덱스가 기본으로 지정됩니다.


#### 3. update by query

update by query와 delete by query는 앞서 살펴본 bulk API, multi get API 와는 성격이 좀 다른 복수 문서 API 입니다. 먼저 검색 쿼리를 통해 주어진 조건을 만족하는 문서를 찾은 뒤 그 문서를 대상으로 업데이트나 삭제 작업을 실시하는 API 입니다. 동작 특성상 서비스에 일상적으로 사용하기보다는 관리적인 목적으로 호출할 때가 많습니다.

```
POST [인덱스 이름]/_update_by_query
{
	"script": {
		"source": "// ...",
	},
	"query": {
		// ...
	}
}
```

update by query API 는 단건 업데이트 API와 달리 doc을 이용한 업데이트를 지원하지 않습니다. script 를 통한 업데이트만을 지원합니다. 또한 업데이트 API 에서 사용 가능한 painless 스크립트 문맥 정보 중에서 ctx.\_now를 사용할 수 없습니다.

> update by query를 이용한 업데이트 작업은 도중에 충돌이 났거나 다른 문제로 인해 중간에 작업이 중단되더라도 그때까지 업데이트된 내용이 롤백되거나 하지는 않습니다.

##### \[스로틀링]
update by query API는 관리적인 목적으로 수행되는 경우가 많다. 문제가 생긴 데이터를 일괄적으로 처리하거나 변경된 비즈니스 요건에 맞게 데이터를 일괄 수정하는 작업 등에 많이 활용됩니다. 그런데 이런 대량 작업을 수행하면 운영 중인 기존 서비스에도 영향을 줄 수 있습니다. 그러한 상황을 피하기 위해 update by query에는 스로틀링 기능이 있습니다. 적절한 스로틀링 적용을 통해 작업의 속도를 조정하고 클러스터 부하와 서비스 영향을 최소화할 수 있습니다.

```
POST bulk_test/_update_by_query?scroll_size=1000&scroll=1m&requests_per_seconds=500
{
	// ...
}
```

1. scroll_size
	- update by query는 업데이트 전 먼저 검색을 수행합니다.
	- 기본적으로 한 번의 검색 수행에 1000개의 문서를 가져온 뒤 1000개 문서에 대한 업데이트를 수행합니다.
	- 해당 작업이 완료되면 다음 검색을 수행하여 1001번부터 2000번 까지의 문서 1000개를 가져오고 그 1000개의 문서에 대해 업데이트를 수행합니다.
	- scroll_size는 이 스크롤링 단위를 조정하는 설정입니다.

2. scroll
	- update by query가 처음 검색을 수행하면 일종의 스냅샷을 찍습니다.
	- 검색 조건을 만족한 모든 문서를 대상으로 검색이 처음 수행됐을 당시 상태를 검색 문맥(search context)에 보존합니다.
	- 이 검색 문맥을 얼마나 보존할지 지정하는 것이 scroll 설정입니다.
	- 위 쿼리는 1분 동안 검색 문맥이 유지된다는 의미입니다.
	- 모든 작업이 종료될 때까지 필요한 시간을 지정하는 것이 아니라 한 배치 작업에 필요한 시간을 지정하면 됩니다.
	- 즉, 해당 시간 동안 scroll_size 만큼의 작업 처리가 가능해야 하므로 너무 짧은 값을 지정하면 안됩니다.

3. requests_per_second
	- 평균적으로 초당 몇 개까지의 작업을 수행할 것인지를 지정합니다.
	- 실제 동작은 일단 scroll_size 단위로 업데이트 작업을 수행한 뒤 requests_per_second 값에 맞도록 일정 시간을 대기하는 방식으로 진행됩니다.
	- requests_per_second와 scroll_size 값을 조절하는 것이 스로틀링 조정의 핵심입니다.
	- requests_per_second의 기본값은 -1로 스로틀링을 적용하지 않는 설정입니다.

실제 대형 서비스를 운영하는 중에 대량의 update by query 작업을 수행해야 한다면 대부분은 스로틀링을 적용해서 시간을 넉넉히 잡아 클러스터의 부담과 서비스 리스크를 줄이고 안전하게 작업을 수행했을 것입니다.

> 이러면 작업 시간이 길어질텐데 HTTP 요청 결과의 응답을 확인하기 어려워집니다. 이런 문제를 해결하기 위해 tasks API를 사용해서 update by query를 비동기적으로 요청할 수 있습니다.


##### \[비동기적 요청과 tasks API]
엘라스틱서치에서 update by query 요청 시에는 wait_for_completion 매개변수를 false로 지정하면 비동기적 처리를 할 수 있습니다. 해당 요청을 받으면 엘라스틱서치는 작업을 task로 등록한 뒤 즉시 task의 id가 포함된 응답을 반환합니다.

이 값은 노드의 id와 해당 노드 내 task의 id를 콜론(:)으로 연결한 형태입니다. 클라이언트는 이 값을 가지고 tasks API를 호출함으로써 작업의 진행 경과를 확인하거나 작업 진행을 취소할 수 있습니다.


엄밀히 말하면 모든 수행 중인 update by query 는 wait_for_completion 값에 상관없이 모두 task의 형태로 동작하며 tasks 조회 API를 통해 작업 진행을 확인할 수 있습니다. 다만 wait_for_completion 을 false로 지정하면 이 진행 상황이 .tasks 라는 내부 인덱스에 문서로 저장됩니다.

아래 요청을 통해 작업 내용을 확인할 수 있습니다.

```
GET .tasks/_cod/[task id]
GET _tasks/[task id]
```

이전에는 전자의 API를 사용했지만 점차 후자의 API로 넘어가는 추세입니다.

아래 요청을 통해 작업 진행 중 문제가 발생했다면 작업을 취소할 수 있습니다.

```
POST _tasks/[task id]/_cancel
```


서비스를 운영하는 중에 대량 작업을 수행할 때는 예상치 못한 문제를 만날 수 있습니다. 예를 들어 일괄 업데이트에 적절한 스로틀링을 적용해 작업을 진행 중이었지만 외부적인 이슈로 인해 나중에 서비스 트래픽이 급증하는 문제가 발생할 수 있습니다. 이런 경우 작업을 72시간 수행 했더라도 취소할지 아니면 2시간만 더 작업하면 끝나서 버틸지 판단하기 어렵습니다.

일반적으로 서비스 중요도에 따라 다르겠지만 안정적인 방법은 72시간 동안 수행한 작업이 있더라도 일단 취소하는 것입니다. 무엇보다 update by query는 중간에 작업을 그만두더라도 RDB 처럼 기존 변경분을 롤백시키지 않습니다. 취소한 후 검색 조건을 수정해 아직 변경되지 않은 문서 만을 대상으로 업데이트를 수행하는 것이 좋습니다.

한편 작업 특성에 따라 아직 변경되지 않은 문서를 걸러내기가 매우 어려울 수도 있는데, 이럴 때 선택할 수 있는 수로 스로틀링을 동적으로 변경하는 방법이 있습니다.

```
POST _update_by_query/[task id]/_rethrottle?requests_per_second=[변경할 값]
```

위와 같이 \_rethrottle을 이용하면 작업의 스로틀링을 동적으로 변경할 수 있어 문제 상황에 유연하게 대처가 가능합니다.


마지막으로 wait_for_completion=false를 통해 .tasks 인덱스에 등록된 작업이 성공하거나 취소됐는지의 여부와 상관없이 등록된 작업 결과는 엘라스틱서치에 계속 남습니다. 작업의 상황을 충분히 확인했다면 다음과 같이 .tasks 인덱스에서 문서를 삭제하면 좋습니다.

```
DELETE .tasks/_doc/[task id]
```


##### \[슬라이싱]
관리적 목적의 대량 업데이트를 수행하는 경우, 스로틀링을 적용해 부하를 줄이는 선택도 있겠지만 반대로 업데이트 성능을 최대로 끌어내 빠른 시간 안에 끝내고자 하는 선택도 있습니다. 정기점검을 걸고 서비스 요청을 차단한 다음 공지한 시간 내에 작업을 끝내야 하는 상황이 대표적입니다. 이럴 때 사용할 수 있는 방법이 슬라이싱입니다.

slices 매개변수를 지정하면 검색과 업데이트를 지정한 개수로 쪼개 병렬적으로 수행합니다.

```
POST [인덱스 이름]/_update_by_query?slices=auto
{
	// ...
}
```

slices의 기본값은 1로, 작업을 병렬로 쪼개지 않는다는 뜻입니다. 위 예처럼 slices의 값을 auto로 지정하면 엘라스틱서치가 적절한 개수를 지정해서 작업을 병렬 수행합니다. 보통은 지정한 인덱스의 주 샤드 수가 슬라이스의 수가 됩니다.

물론 slices 값을 auto로 지정하지 않고 숫자를 직접 지정할 수도 있습니다. 그러나 이 경우 슬라이스의 수가 주 샤드 수를 넘는 경우 성능이 급감할 수 있다는 점을 유의해야 합니다.

> requests_per_second 옵션은 각 슬라이스에 쪼개져서 적용됩니다.


#### 4) delete by query

delete by query 는 update by query 처럼 먼저 지정한 검색 쿼리로 삭제할 대상을 지정한 뒤에 삭제를 수행하는 작업입니다. update by query 와 같이 관리적인 목적으로 많이 사용합니다. 그러나 update by query와 다르게 주기적인 배치성 작업으로 수행하는 경우도 많습니다.

> 특히 오래된 데이터 등 더 이상 사용하지 않는 데이터를 삭제하는 경우에 많이 사용합니다.

```
POST [인덱스 이름]/_delete_by_query
{
	"query": {
		// ...
	}
}
```



---


## 3. 검색 API

엘라스틱서치의 기본이자 핵심은 검색 엔진입니다.

#### 1) 검색 대상 지정

```
GET [인덱스 이름]/_search
POST [인덱스 이름]/_search
GET _search
POST _search
```

GET과 POST 중 무엇을 사용해도 동작은 동일합니다. 인덱스 이름을 지정하지 않으면 전체 인덱스에 대해 검색합니다.


#### 2) 쿼리 DSL 검색과 쿼리 문자열 검색

##### \[쿼리 DSL 검색]
쿼리 DSL을 사용하는 방법은 지금까지 학습하면서 사용해온 방식입니다.

```
GET my_index/_search
{
	"query": {
		"match": {
			"title": "hello"
		}
	}
}
```

요청 본문에 query 필드를 넣어 그 안에 원하는 쿼리와 질의어를 기술합니다.


##### \[쿼리 문자열 검색]

검색 동작을 루씬 쿼리 문자열을 지정하는 방법으로 수행합니다.

```
GET my_index/_search?q=title:hello
```


##### \[match_all 쿼리]
match_all 쿼리는 모든 문서를 매치하는 쿼리입니다. query 부분을 비워 두면 기본값으로 지정되는 쿼리입니다.


##### \[match 쿼리]
match 쿼리는 지정한 필드의 내용이 질의어아 매치되는 문서를 찾는 쿼리입니다. 필드가 text 타입이라면 필드의 값도 질의어도 모두 애널라이저로 분석됩니다.

match 쿼리의 기본 동작은 OR 조건으로 동작합니다. operator를 and로 지정하면 모든 텀이 매치된 문서만 검색되도록 변경할 수 있습니다.


##### \[term 쿼리]
term 쿼리는 지정한 필드의 값이 질의어와 정확히 일치하는 문서를 찾는 쿼리입니다. 대상 필드에 노멀라이저가 지정돼 있다면 질의어도 노멀라이저 처리를 거칩니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		"term": {
			"fieldName": {
				"value": "hello"
			}
		}
	}
}
```

term 쿼리를 문자열 필드를 대상으로 사용할 때는 keyword 타입과 잘 맞습니다. 질의어도 문서도 같은 노멀라이저 처리를 거치므로 직관적으로 사용할 수 있습니다.

text 타입의 필드를 대상으로 term 쿼리를 사용하는 경우는 약간 다릅니다. 질의어는 노멀라이저 처리를 거치지만 필드의 값은 애널라이저로 분석한 뒤 생성된 역색인을 이용하게 됩니다. 분석 결과 단일 텀이 생성됐고 그 값이 노멀라이저를 거친 질의어와 완전히 같은 경우에만 검색에 걸릴 것입니다.


##### \[terms 쿼리]
terms 쿼리는 term 쿼리와 매우 유사합니다. 지정할 필드의 값이 질의어와 정확히 일치하는 문서를 찾는 쿼리입니다. 대신 질의어를 여러 개 지정할 수 있으며, 하나 이상의 질의어가 일치하면 검색 결과에 포함됩니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		"terms": {
			"fieldName": ["hello", "world"]
		}
	}
}
```


##### \[range 쿼리]
range 쿼리는 지정한 필드의 값이 특정 범위 내에 있는 문서를 찾는 쿼리입니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		"range": {
			"fieldName": {
				"gte": 100,
				"lt": 200
			}
		}
	}
}
```

범위는 gt, lt, gte, lte를 이용하여 지정합니다.

> 엘라스틱서치는 문자열 필드를 대상으로 한 range 쿼리를 부하가 큰 쿼리로 분류합니다. 그러므로 range 쿼리는 데이터 양상을 파악하고 부담이 없는 상황에서만 사용해야 합니다.

range 쿼리의 대상 필드가 date 타입이면 간단한 날짜 시간 계산식도 사용할 수 있습니다.


##### \[prefix 쿼리]
prefix 쿼리는 필드의 값이 지정한 질의어로 시작하는 문서를 찾는 쿼리입니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		"prefix": {
			"fieldName": {
				"value": "hello"
			}
		}
	}
}
```

prefix도 무거운 쿼리로 분류됩니다. 다만 prefix 쿼리는 쿼리 문자열 검색에서 소개했던 와일드카드 검색처럼 아예 사용하지 말아야 할 정도까지는 아니다. 데이터 규모와 쿼리의 파급력에 대해 충분히 파악이 된 상태라면 관리를 위해 사용하는 단발성 쿼리 정도는 감수할 만한 성능이 나옵니다.

> 그러나 일상적으로 호출되는 서비스성 쿼리로는 적절하지 못합니다.

만약 prefix 쿼리를 서비스 호출 용도로 사용하려 한다면 매핑에 index_prefixes 설정을 넣는 방법이 있습니다.

```
PUT prefix_mapping_test
{
	"mappings": {
		"properties": {
			"prefixField": {
				"type": "text",
				"index_prefixes": {
					"min_chars": 3,
					"max_chars": 5
				}
			}
		}
	}
}
```

위와 같이 매핑에 index_prefixes를 지정하면 엘라스틱서치는 문서를 색인할 때 min_chars와 max_chars 사이의 prefix를 미리 별도 색인합니다. 색인 크기와 색인 속도에서 손해를 보는 대신 prefix 쿼리의 성능을 높이는 방법입니다.


##### \[exists 쿼리]
exists 쿼리는 지정한 필드를 포함한 문서를 검색합니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		"exists": {
			"field": "fieldName"
		}
	}
}
```


##### \[bool 쿼리]
bool 쿼리는 여러 쿼리를 조합하여 검색하는 쿼리입니다. must, must_not, filter, should의 4가지 종류의 조건절에 다른 쿼리를 조합하여 사용합니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		"bool": {
			"must": [
				{"term": {"field1": {"value": "hello"}}},
				{"term": {"field2": {"value": "world"}}}
			],
			"must_not": [
				{"term": {"field4": {"value": "elasticsearch-test"}}}
			],
			"filter": [
				{"term": {"field3": {"value": true}}}
			],
			"should": [
				{"match": {"field4": {"query": "elasticsearch"}}},
				{"match": {"field5": {"query": "lucene"}}}
			],
			"minimum_should_match": 1
		}
	}
}
```

- must 조건절과 filter 조건절에 들어간 하위 쿼리는 모두 AND 조건으로 만족해야 최종 검색 결과에 포함됩니다. 
- must_not 조건절에 들어간 쿼리를 만족하는 문서는 최종 검색 결과에서 제외됩니다.
- should 조건절에 들어간 쿼리는 minimum_should_match에 지정한 개수 이상의 하위 쿼리를 만족하는 문서가 최종 검색 결과에 포함됩니다.


< 쿼리 문맥과 필터 문맥 >
must와 filter는 모두 AND 조건으로 검색을 수행하지만 점수를 계산하느냐 여부가 다릅니다.
filter 조건에 들어간 쿼리는 단순히 문서의 매치 여부만을 판단하고 랭킹에 사용할 점수를 매기지 않습니다. 이렇게 점수를 매기지 않고 단순히 조건을 만족하는지 여부만을 참과 거짓으로 따지는 검색과정을 <span style="color:#ff0000">필터 문맥</span> 이라고 합니다. 문서가 주어진 검색 조건을 얼마나 더 잘 만족하는지 유사도 점수를 매기는 검색 과정은 <span style="color:#ff0000">쿼리 문맥</span> 이라고 합니다.

조건을 만족하는지 여부만이 중요하고 최종 검색 결과에서 랭킹에 영향을 주기 위해 유사도 점수를 매길 의미가 없는 조건이라면 필터 문맥으로 검색을 해야 성능상 유리합니다.

> 점수를 계산하는 비용을 아끼는 것 뿐만 아니라 필터 문맥으로 검색한 결과는 쿼리 캐시에 저장해 재활용할 수 있기 때문입니다.

![[Pasted image 20240131000825.png]]


< 쿼리 수행 순서 >
must, filter, must_not, should 사이에서 어떤 쿼리가 먼저 수행된다는 규칙은 없습니다. 엘라스틱서치는 검색 요청을 받으면 내부적으로 쿼리를 루씬의 여러 쿼리로 쪼갠 뒤 조합하여 재작성합니다. 그 뒤 쪼개진 각 쿼리를 수행할 경우 비용이 얼마나 소모되는지 내부적으로 추정합니다. 추정한 비용과 효과를 토대로 유리할 것으로 생각되는 부분을 먼저 수행합니다.

또한 하부 쿼리 하나를 통째로 수행한 뒤 다음 하부 쿼리 하나를 수행하는 것도 아닙니다. 여러 쿼리를 만족하는 문서 후보들을 뽑아 놓고 문서에 대한 실제 조건 만족 여부를 하나씩 체크하는 과정에서 여러 하부 쿼리를 병렬적으로 수행하기도 합니다.

쿼리 문맥으로 검색을 수행하든 필터 문맥으로 검색을 수행하든 매치된 문서를 찾는 수행 순서에는 영향이 없습니다. 쿼리 문맥과 필터 문맥 모두 동일하게 매치되는 문서를 참과 거짓으로 판별하는 과정을 거치고 점수를 계산하는 과정은 모든 문서의 매치를 다 확인한 이후에 수행합니다. 이 과정에서 필터 문맥의 쿼리에는 점수 계산을 하지 않을 뿐입니다.

> 쿼리 문맥의 쿼리에 매치된 모든 문서에 대해 유사도 점수를 계산하지 않습니다.


##### \[constant_score 쿼리]
constant_score 쿼리는 하위 filter 부분에 지정한 쿼리를 필터 문맥에서 검색하는 쿼리입니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		"constant_score": {
			"filter": {
				"term": {
					"fieldName": "hello"
				}
			}
		}
	}
}
```



##### \[그 외 주요 매개변수]

- 라우팅
검색 API도 색인 API나 조회 API와 마찬가지로 라운팅을 지정해 주는 것이 좋습니다.

```
GET [인덱스 이름]/_search?routing=[라우팅]
{
	"query": {
		// ...
	}
}
```

라우팅을 지정하지 않으면 전체 샤드에 검색 요청이 들어갑니다. 반면 라우팅을 지정하면 정확히 한 샤드에만 검색 요청이 들어가므로 성능상 이득이 매우 큽니다.

> 라우팅 지정 여부가 가져오는 성능 차이가 크기 때문에 가능하다면 인덱스 설계 단계부터 서비스와 데이터 특성을 면밀히 파악한 후 최대한 라우팅 이득을 볼 수 있도록 설계하는 편이 좋습니다.


- explain
explain을 사용하면 검색을 수행하는 동안 쿼리의 각 하위 부분에서 점수가 어떻게 계산됐는지 설명해 줍니다. 그러므로 디버깅 용도로 사용할 수 있습니다.

```
GET [인덱스 이름]/_search?explain=true
{
	"query": {
		// ...
	}
}
```

이렇게 요청을 하면 응답 내용의 \_explanation 부분에서 점수 계산 과정의 상세한 설명을 확인할 수 있습니다. 다만 이 설명을 만들기 위해 내부적으로 쿼리를 덜 최적화해 수행하기도 합니다. 이 때문에 explain 옵션을 주지 않고 검색했을 때보다 성능이 하락할 수 있습니다. 따라서 서비스에서 일상적으로 explain을 켜는 것보다는 디버깅 용도로만 사용하는 것이 좋습니다.


- search_type
search_type을 지정하면 유사도 점수를 계산할 때 각 샤드 레벨에서 계산을 끝낼지 여부를 선택할 수 있습니다.

```
GET [인덱스 이름]/_search?search_type=dfs_query_then_fetch
{
	"query": {
		// ...
	}
}
```

지정할 수 있는 값은 query_then_fetch와 dfs_query_then_fetch로 두 가지입니다.

query_then_fetch: 기본 설정입니다. 각 샤드 레벨에서 유사도 점수 계산을 끝냅니다. 점수 계산이 약간 부정확할 수는 있지만 검색 성능의 차이가 크기 때문에 특별한 사유가 없다면 이 방법을 사용할 것을 추천합니다.

dfs_query_then_fetch: 모든 샤드로부터 정보를 모아 유사도 점수를 글로벌하게 계산합니다. 점수 계산의 정확도는 올라가지만 검색 성능이 떨어지게 됩니다.


##### \[검색 결과 정렬]
검색 API 호출 시 요청 본문에 sort를 지정하면 검색 결과를 정렬할 수 있습니다. 정렬에 사용할 필드 이름과 오름차순 또는 내림차순 종류를 지정하면 됩니다.

```
GET [인덱스 이름]/_search
{
	"query": {
		// ...
	},
	"sort": [
		{ "field1": {"order": "desc" } },
		{ "field2": {"order": "asc" } },
		"field3"
	]
}
```

위와 같이 정렬 대상 필드를 여럿 지정할 수도 있습니다. 이런 경우 요청에 지정한 순서대로 정렬을 수행합니다. field3 처럼 필드 이름만 명시하면 내림차순으로 정렬합니다.

엘라스틱서치의 필드타입 유형 중에는 정렬에 사용할 수 있는 타입과 불가능한 타입이 있습니다. 숫자 타입, date 타입, boolean 타입, keyword 타입은 정렬 대상이 될 수 있지만 text 타입은 정렬 대상으로 지정할 수 없습니다.

필드 이름 외에도 \_score 나 \_doc 을 지정할 수 있습니다. \_score는 검색을 통해 계산된 유사도 점수로 정렬합니다. sort 옵션을 별도로 지정하지 않았을 때의 기본 정렬이 바로 \_score 내림차순 정렬입니다. \_doc은 문서 번호 순서로 정렬합니다. 

정렬 수행 중에는 필드의 값이 메모리에 올라갑니다. 서비스에서 지속적으로 정렬된 검색을 사용한다면 정렬 대상이 될 필드는 좀 더 메모리를 적게 차지하는 integer, short, float 등의 타입으로 설계하는 것도 좋습니다.

> 참고로 정렬 옵션에 \_score가 포함되지 않은 경우 엘라스틱서치는 유사도 점수를 계산하지 않습니다.


##### \[페이지네이션]
페이지네이션에는 기본적으로 from과 size / 검색에 매칭되는 전체 문서를 모두 확인할 때 사용하는 scroll / 성능 부담은 상대적으로 낮춘 채 본격적인 페이지네이션을 할 수 있는 search_after 가 있습니다.

- from 과 size
size는 검색 API의 결과로 몇 개의 문서를 반환할 것인지 지정합니다. fro은 몇 번째 문서부터 결과를 반환할지 그 오프셋을 지정합니다.

```
GET [인덱스 이름]/_search
{
	"from": 10,
	"size": 5,
	"query": {
		// ...
	}
}
```

from의 기본값은 0, size의 기본 값은 10 입니다. 이 from과 size는 제한적으로 사용해야 하며 본격적인 페이지네이션에는 사용할 수 없다. 특정 시점의 데이터를 중복이나 누락 없이 엄밀하게 페이지네이션하여 제공해야 한다면 from과 size는 사용하지 말아야 합니다.

> 만약 위 쿼리 이후 from 15, size 5로 지정해 검색하면 내부적으로는 상위 20개의 문서를 수집하는 검색을 다시 수행한 뒤 마지막에 결과의 일부를 잘라내서 반환하는 방식으로 동작합니다.

본격적인 페이지네이션을 해야 한다면 scroll 이나 search_after를 사용해야 합니다.


- scroll
scroll은 검색 조건에 매칭되는 전체 문서를 모두 순회해야 할 때 적합한 방법입니다. 스크롤을 순회하는 동안에는 최초 검색 시의 문맥(search context)이 유지됩니다. 중복이나 누락도 발생하지 않습니다.

```
GET [인덱스 이름]/_search?scroll=1m
{
	"size": 1000,
	"query": {
		// ...
	}
}
```

검색 결과에서 \_scroll_id를 확인할 수 있는데 다음 검색에서 이 값을 이용해서 검색을 수행하면 더 이상 문서가 반환되지 않을 때까지 scroll 검색을 반복합니다.

```
GET _search/scroll
{
	"scroll_id": "이전 검색에서 받은 scroll_id 값",
	"scroll": "1m"
}
```

scroll 검색을 한 번 수행할 때마다 검색 문맥이 연장됩니다. 즉, scroll 매개변수에 지정한 검색 문맥의 유지 시간은 배치와 배치 사이를 유지할 정도의 시간으로 지정하면 됩니다.

세그먼트 병합이 끝나서 더 이상 필요 없는 세그먼트도 검색 문맥이 유지되는 동안은 삭제되지 않고 유지됩니다. 보존된 검색 문맥은 유지 시간이 종료되면 자동으로 삭제되지만, 좀 더 빠른 자원의 반납을 위해 다음과 같이 명시적으로 제거해줄 수도 있습니다.

```
DELETE _search/scroll
{
	"scroll_id": "삭제하려는 scroll_id 값"
}
```

scroll은 검색 문맥을 보존한 뒤 전체 문서를 순회하는 동작 특성상 검색 결과의 정렬 여부가 상관 없는 작업에 사용하는 경우가 많습니다. 그렇다면 \_doc 로 정렬을 지정하는 것이 좋은데, 이렇게 지정하면 유사도 점수를 계산하지 않기 때문에 정렬을 위한 별도의 자원도 사용하지 않기 때문입니다.

scroll은 서비스에서 지속적으로 호출하는 것을 의도하고 만들어진 기능이 아닙니다. 주로 대량의 데이터를 다른 스토리지로 이전하거나 덤프하는 용도로 사용합니다. 서비스에서 사용자가 지속적으로 호출하기 위한 페이지네이션 용도로는 바로 이어 소개할 search_after를 사용하는 것이 좋습니다.


- search_after
서비스에서 사용자가 검색 결과를 요청케 하고 결과에 페이지네이션을 제공하는 용도라면 search_after를 사용하는 것이 가장 적합합니다. search_after에는 sort를 지정해야 합니다.

> 동일한 정렬 값이 등장할 수 없도록 최소한 1개 이상의 동점 제거용 필드를 지정해야 합니다.

```
GET kibana_sample_data_ecommerce/_search
{
	"size": 20,
	"query": {
		"term": {
			"currency": {
				"value": "EUR"
			}
		}
	},
	"sort": [
		{
			"order_date": "desc"
		},
		{
			"order_id": "asc"
		}
	]
}
```

첫 번째 검색이 끝나면 검색 결과의 가장 마지막 문서에 표시된 sort 기준값을 가져와 search_after 부분에 넣어 그다음 검색을 요청합니다.

```
GET kibana_sample_data_ecommerce/_search
{
	"size": 20,
	"query": {
		// ...
	},
	"search_after": [ 기존 검색 결과 order_date 마지막 값, 기존 검색 결과 order_id 마지막 값],
	"sort": [
		{
			"order_date": "desc"
		},
		{
			"order_id": "asc"
		}
	]
}
```

당연하지만 동점 제거용 필드에는 문서를 고유하게 특정할 수 있는 값이 들어아야 합니다. 그러나 \_id 값을 동점 제거용 기준 필드로 사용하는 것은 좋지 않습니다. \_id 필드는 doc_values가 꺼져 있기 때문에 이를 기준으로 하는 정렬은 많은 메모리를 사용하게 됩니다.

> \_id 필드값과 동일한 값을 별도의 필드에 저장해 뒀다가 동점 제거용으로 사용하는 편이 낫습니다.

search_after를 사용할 때 인덱스 상태를 특정 시점으로 고정하려면 point in time API를 함께 조합해서 사용하면 됩니다.


- point in time API

point in time API느ㄴ 검색 대상의 상태를 고정할 때 사용합니다. keep_alive 매개변수에 상태를 유지할 시간을 지정합니다.

```
POST kibana_sample_data_ecommerce/_pit?keep_alive=1m
```

위 요청을 보내면 pit id 값을 얻을 수 있고 이를 search_after와 같은 곳에 활용할 수 있습니다.

```
GET _search
{
	"size": 20,
	"query": {
		// ...
	},
	"pit": {
		"id": "전달받은 pit_id",
		"keep_alive": "1m"
	},
	"sort": [
		{
			"order_date": "desc"
		}
	]
}
```

pit 부분에 얻어온 pit id를 지정했습니다. 그리고 요청 uri을 확인해보면 검색 대상이 될 인덱스를 지정하지 않았다는 것을 알 수 있습니다. pit를 지정하는 것 자체가 검색 대상을 지정하는 것이기 때문입니다. 아래 요청은 pit와 search_after를 같이 사용한 것입니다.

```
GET _search
{
	"size": 20,
	"query": {
		// ...
	},
	"pit": {
		"id": "전달받은 pit_id",
		"keep_alive": "1m"
	},
	"search_after": [ 17234, 231 ],
	"sort": [
		{
			"order_date": "desc"
		}
	]
}
```

pit도 다 사용한 뒤에는 명시적으로 삭제해줄 수 있습니다.

```
DELETE _pit
{
	"id": "pit id 값"
}
```



---


## 4. 집계

엘라스틱서치는 검색을 수행한 뒤 그 결과를 집계(aggregation)하는 다양한 방법을 제공합니다. 검색이 엘라스틱서치의 뿌리였다면 집계는 꽃입니다.


#### 1)집계 기본

엘라스틱서치의 집계는 검색의 연장선입니다. 집계의 대상을 추려낼 검색 조건을 검색 API에 담은 뒤 집계 조건을 추가해서 호출합니다. 가장 간단한 형태인 sum 집계를 살펴보겠습니다.

```
GET kibana_sample/_search
{
	"size": 0,
	"query": {
		"term": {
			"currency": {
				"value": "EUR"
			}
		}
	},
	"aggs": {
		"my-sum-aggregation-name": {
			"sum": {
				"field": "taxless_total_price"
			}
		}
	}
}
```

위 쿼리의 결과로 아래 사진과 같은 응답을 받을 수 있습니다.

![[Pasted image 20240201194022.png]]

요청을 보면 검색 API의 요청 본문에 aggs 만 추가된 것을 확인할 수 있습니다. 요청 본문에서 size를 0으로 지정한 부분에 주목해보면, size를 0으로 지정하면 검색에 상위 매칭된 문서가 무엇인지 받아볼 수 없습니다. 하지만 이와 상관없이 검색 조건에 매치되는 모든 문서는 집계 작업에 사용됩니다.

> 어짜피 집계 작업의 핵심은 집계 연산의 결과이지 매치된 문서가 아닙니다. 즉, 대부분의 집계 요청에서 size를 0으로 지정하는 것이 이득입니다.

size가 0이면 각 샤드에서 수행한 검색 결과에서 상위 문서의 내용을 수집해 모을 필요가 없고 점수를 계산하는 과정도 수행하지 않습니다. 이로 인해 성능에 이득이 있고 캐시의 도움도 더 많이 받을 수 있습니다.

집계 요청의 상세는 aggs 밑에 기술합니다. 요청 한 번에 여러 집계를 요청할 수도 있기 때문에 결과에서 이들을 구분할 수 있도록 집계에 이름을 붙여야 합니다.

집계 작업은 검색 쿼리에 매칭된 모든 문서에 대해 수행합니다. 이를 염두에 두지 않으면 과도한 양의 데이터를 대상으로 집계를 수행할 수 있습니다. 과도한 집계는 전체 클러스터 성능을 급격히 저하시킵니다.

사용자는 집계에 사용할 시계열 데이터 범위를 과도하게 늘리거나 대시보드 화면에서 변화하는 데이터를 실시간으로 확인하기 위해 과도한 리프레시를 수행하는 등의 작업 시도가 어떤 리스크를 가지는지 모두 인지하고 있어야 합니다.


#### 2) 메트릭 집계

메트릭 집계는 문서에 대한 산술적인 연산을 수행합니다.

##### \[avg, max, min, sum 집계]
avg, max, min, sum 집계는 검색에 매칭된 문서를 대상으로 지정한 필드의 값을 가져온 뒤 각각 평균, 최댓값, 최솟값, 합을 계산하여 반환합니다.


##### \[stats 집계]
stats 집계는 지정한 필드의 평균, 최댓값, 최솟값, 합, 개수를 모두 계산해서 반환합니다.

```
GET kibana_sample/_search
{
	"size": 0,
	"query": {
		// ...
	},
	"aggs": {
		"my-stats-aggregation-name": {
			"stats": {
				"field": "taxless_total_price"
			}
		}
	}
}
```

위 쿼리의 결과는 다음과 같습니다.

![[Pasted image 20240201194826.png]]

이렇게 여러 숫자 값을 한꺼번에 반환하는 메트릭 집계를 다중 값 숫자 메트릭 집계(multi-value numeric metric aggregation)라고 부릅니다.


##### \[cardinality 집계]
cardinality 집계는 지정한 필드가 가진 고유한 값의 개수를 계산해 반환합니다. 이 값은 HyperLogLog++ 알고리즘을 사용해 추정한 근사값입니다.

```
GET kibana_sample/_search
{
	"size": 0,
	"query": {
		// ...
	},
	"aggs": {
		"my-cardinality-aggregation-name": {
			"cardinality": {
				"field": "customer_id",
				"precision_threshold": 3000
			}
		}
	}
}
```

위 쿼리의 결과는 아래와 같습니다.

![[Pasted image 20240201195045.png]]

precision_threshold 옵션은 정확도를 조절하기 위해 사용합니다. 이 값을 높이면 정확도가 올라가지만 그만큼 메모리를 더 사용합니다. 다만 정확도를 올리기 위해 이 값을 무작정 많이 높일 필요는 없습니다.

> precision_threshold가 최종 cardinality보다 높다면 정확도가 충분히 높습니다.

이 옵션의 기본값은 3000이며 최댓값은 40000입니다. 지정된 필드의 cardinality가 높고 낮은과 상관없이 메모리 사용량은 precision_threshold에만 영향받습니다.


#### 3) 버킷 집계

버킷 집계는 문서를 특정 기준으로 쪼개어 여러 부분 집합으로 나눕니다. 이 부분 집합을 버킷이라고 합니다. 또한 각 버킷에 포함된 문서를 대상으로 별도의 하위 집계(sub-aggregation)를 수행할 수 있습니다.

##### \[range 집계]
range 집계는 지정한 필드값을 기준으로 문서를 원하는 버킷 구간으로 쪼갭니다. 버킷 구간을 나눌 기준이 될 필드와 기준값을 지정해 요청합니다.

```
GET kibana_sample_data_flights/_search
{
	"size": 0,
	"query": {
		"match_all": {}
	},
	"aggs": {
		"distance-kilometers-range": {
			"range": {
				"field": "DistanceKilometers",
				"ranges": [
					{
						"to": 5000
					},
					{
						"from": 5000,
						"to": 10000
					},
					{
						"from": 10000
					}
				]
			},
			"aggs": {
				"average-ticket-price": {
					"avg": {
						"field": "AvgTicketPrice"
					}
				}
			}
		}
	}
}
```

위 쿼리의 결과는 아래와 같습니다.

![[Pasted image 20240201195600.png]]

요청 본문을 잘 보면 range 밑에 aggs가 하나 더 들어가 있는 것을 확인할 수 있습니다. 이 부분이 하위 집계입니다. 위 쿼리를 이용해 3개의 버킷 안에 있는 문서를 대상으로 각각 AvgTicketPrice의 평균을 구할 수 있습니다.

버킷 집계의 활용 핵심은 이 하위 집계에 있습니다. 문서 전체에 대해 하나의 집계를 수행하는 것이 아니라 이렇게 문서를 여러 구간의 버킷으로 나눈 뒤 각 버킷에 대해서 하위 집계를 수행하도록 하는 것입니다. 하위 집계에 또 버킷 집계를 넣으면 다시 그 하위 집계를 지정하는 것도 가능합니다. 그러나 하위 집계의 깊이가 너무 깊어지면 성능에 심각한 문제가 생기니 적당히 지정해야 합니다.

##### \[date_range 집계]
date_range 집계는 range 집계와 유사하나 date 타입 필드를 대상으로 사용한다는 점, from과 to에 간단한 날짜 시간 계산식을 사용할 수 있다는 점에서 차이가 있습니다.

```
GET kibana_sample/_search
{
	"size": 0,
	"query": {
		// ...
	},
	"aggs": {
		"date-range-aggs": {
			"date_range": {
				"field": "order_date",
				"ranges": [
					{
						"to": "now-10d/d"
					},
					{
						"from": "now-10d/d",
						"to": "now"
					},
					{
						"from": "now"
					}
				]
			}
		}
	}
}
```

새로운 데이터가 들어와서 인덱스의 상태가 달라지면 샤드 요청 캐시는 무효화되기 때문에 고정된 인덱스가 아니면 캐시 활용도가 떨어집니다. 게다가 완전히 같은 집계를 여러 번 요청해야 하는 상황도 많지 않기 때문에 이 점을 크게 신경 쓸 필요는 없습니다. 다만 now가 포함된 집계는 샤드 요청 캐시에 올라가지 않는다는 점을 인지하고 사용해야 합니다.

##### \[histogram 집계]
histogram 집계는 지정한 필드의 값을 기준으로 버킷을 나눈다는 점에서 range 집계와 유사합니다. 다른 점은 버킷 구분의 경계 기준값을 직접 지정하는 것이 아니라 버킷의 간격을 지정해서 경계를 나눈다는 점입니다.

```
GET kibana_sample/_search
{
	"size": 0,
	"query": {
		// ...
	},
	"aggs": {
		"my-histogram": {
			"histogram": {
				"field": "DistanceKillometers",
				"interval": 1000
			}
		}
	}
}
```

위 쿼리의 결과는 아래와 같습니다.

![[Pasted image 20240201200714.png]]

이렇게 interval을 지정하면 해당 필드의 최솟값과 최댓값을 확인한 후 그 사이를 interval에 지정한 간격으로 쪼개서 버킷을 나눕니다. 특별히 지정하지 않으면 기본적으로 0을 기준으로 히스토그램의 계급을 나눕니다. 기준을 조정하고 싶을 때는 offset을 사용할 수 있습니다.

이 밖에도 min_doc_count를 지정해서 버킷 내 문서 개수가 일정 이하인 버킷은 결과에서 제외할 수 있습니다. 위 요청에 "min_doc_count": 600 설정을 추가 지정했다면 "key": 2000, 3000 구간의 버킷은 결과에서 제외됐을 것입니다.

##### \[date_histogram 집계]
date_histogram 집계는 histogram 집계와 유사하지만 대상으로 date 타입 필드를 사용한다는 점이 다릅니다. 또한 interval 대신에 calendar_interval이나 fixed_interval을 사용합니다.

```
GET kibana_sample/_search
{
	"size": 0,
	"query": {
		// ...
	},
	"aggs": {
		"my-date-histogram": {
			"date_histogram": {
				"field": "order_date",
				"calendar_interval": "day"
			}
		}
	}
}
```

위 요청에 대한 응답은 아래 사진과 같습니다.

![[Pasted image 20240201201129.png]]

calendar_interval 에는 다음과 같은 값들을 지정할 수 있습니다.

- minute 또는 1m : 분 단위
- hour 또는 1h : 시간 단위
- day 또는 1d : 일 단위
- month 또는 1M : 월 단위
- quarter 또는 1q : 분기 단위
- year 또는 1y : 연 단위

그러나 calendar_interval 에는 12h 처럼 1개 단위가 아닌 값은 지정할 수 없습니다. 이런 단위로 버킷을 나누고 싶을 때는 calendar_interval 대신에 fixed_interval을 사용해서 지정해야 합니다.

fixed_interval 에는 ms, s, m, h, d 단위를 사용할 수 있습니다. 예를 들어 "fixed_interval": "3h" 로 지정하면 3시간 간격으로 버킷 구간을 쪼갭니다. 또한 date_histogram 집계도 histogram 집계처럼 offset과 min_doc_count 설정을 지정할 수 있습니다.

##### \[terms 집계]
terms 집계는 지정한 필드에 대해 가장 빈도수가 높은 term 순서대로 버킷을 생성합니다. 버킷을 최대 몇 개까지 생성할 것인지를 size로 지정합니다.

```
GET kibana_sample_data_logs/_search
{
	"size": 0,
	"query": {
		// ...
	},
	"aggs": {
		"my-terms-aggs": {
			"terms": {
				"field": "host.keyword",
				"size" 10
			}
		}
	}
}
```

위 요청에 대한 응답은 아래 사진과 같습니다.

![[Pasted image 20240201201709.png]]

terms 집계는 각 샤드에서 size 개수만큼 term을 뽑아 빈도수를 셉니다. 각 샤드에서 수행된 계산을 한 곳으로 모아 합산한 후 size 개수만큼 버킷을 뽑습니다. 그러므로 size 개수와 각 문서의 분포에 따라 그 결과가 정확하지 않을 수 있습니다. 각 버킷의 doc_count는 물론 하위 집계 결과도 정환한 수치가 아닐 수 있음을 염두에 두어야 합니다. 특히 해당 필드의 고유한 term 개수가 size 보다 많다면 상위에 뽑혀야 할 term이 최종 결과에 포함되지 않을 수 있습니다.

응답 본문에서 확인할 수 있는 doc_count_error_upper_bound 필드는 doc_count의 오차 상한선을 나타냅니다. 이 값이 크다면 size를 높이는 것을 고려할 수 있습니다. 물론 size를 높이면 정확도는 올라가지만 그마큼 성능이 하락합니다.

sum_other_doc_count 필드는 최종적으로 버킷에 포함되지 않은 문서 수를 나타냅니다. 상위 term에 들지 못한 문서 개수의 총합입니다.

만약 모든 term에 대해서 페이지네이션으로 전부 순회하며 집계를 하려고 한다면 size를 무작정 계속 높이는 것보다는 composite 집계를 사용하는 것이 좋습니다. terms 집계는 기본적으로 상위 term을 뽑아서 집계를 수행하도록 설계됐습니다.

##### \[composite 집계]
composite 집계는 sources로 지정된 하위 집계의 버킷 전부를 페이지네이션을 이용해서 효율적으로 순회하는 집계입니다. 또한 sources에 하위 집계를 여러 개 지정한 뒤 조합된 버킷을 생성할 수 있습니다.

```
GET kibana_sample_data_logs/_search
{
	"size": 0,
	"query": {
		"match_all": {}
	},
	"aggs": {
		"composite-aggs": {
			"composite": {
				"size": 100,
				"sources": [
					{
						"terms-aggs": {
							"terms": {
								"field": "host.keyword"
							}
						}
					},
					{
						"date-histogram-aggs": {
							"date_histogram": {
								"field": "@timestamp",
								"calendar_interval": "day"
							}
						}
					}
				]
			}
		}
	}
}
```

위 쿼리의 결과는 아래 사진과 같습니다.

![[Pasted image 20240201202601.png]]

먼저 요청 본문을 살펴보면 composite 아래의 size는 페이지네이션 한 번에 몇 개의 버킷을 반환할 것인가를 지정합니다. sources에는 버킷을 조합하여 순회할 하위 집계를 지정합니다. 여기에는 모든 종류의 집계를 하위 집계로 지정할 수는 없습니다. terms 집계, histogram 집계, date_histogram 집계 등 일부 집계만 지정할 수 있습니다.

terms 집계에는 size를 지정하지 않았습니다. composite 집계 자체가 버킷 전체를 순차적으로 방문하는 목적의 집계이기 때문에 terms 집계의 size 개념이 필요 없습니다.

after_key 부분에서 확인할 수 있는 조합이 바로 페이지네이션을 위해서 필요한 가장 마지막 버킷의 key 입니다. 이 after_key를 가져와서 다음과 같이 요청하면 작업 결과의 다음 페이지를 조회할 수 있습니다. 처음 요청과 동일하지만 composite 아래에 after 부분이 추가됩니다.


#### 4) 파이프라인 집계

파이프라인 집계는 문서나 필드의 내용이 아니라 다른 집계 결과를 대상으로 합니다. 즉, 다른 집계의 결과를 입력값으로 가져와서 작업을 수행합니다. 주로 buckets_path라는 인자를 통해 다른 집계의 결과를 가져오며, 이 buckets_path는 상대 경로로 지정합니다. 다음은 buckets_path를 지정할 때 사용할 수 있는 대표적인 구문입니다.

- > : 하위 집계로 이동하는 구분자
- . : 하위 메트릭으로 이동하는 구분자
- 집계 이름
- 메트릭 이름

> buckets_path에서는 > 또는 . 문자를 통해 하위 경로로 이동할 수 있으나 상위 경로로는 이동할 수 없습니다.

##### \[comulative_sum 집계]
comulative_sum 집계는 다른 집계의 값을 누적하여 합산합니다. buckets_path로 누적 합산할 집계의 이름을 지정합니다.

```
GET kibana_sample_data_ecommerce/_search
{
	"size": 0,
	"query": {
		"match_all": {}
	},
	"aggs": {
		"daily-timestamp-bucket": {
			"date_histogram": {
				"field": "order_date",
				"calendar_interval": "day"
			},
			"aggs": {
				"daily-total-quantity-average": {
					"avg": {
						"field": "total_quantity"
					}
				},
				"pipeline-sum": {
					"cumulative_sum": {
						"buckets_path": "daily-total-quantity-average"
					}
				}
			}
		}
	}
}
```

위 요청의 결과는 아래 사진과 같습니다.

![[Pasted image 20240201204223.png]]

일 단위로 total_quantity의 평균을 구하기 위해 date_histogram으로 버킷을 나눈 뒤 그 하위 집계로 avg 집계를 지정했습니다. 그리고 date_histogram의 하위 집계로 cumulative_sum 집계를 추가 지정했습니다. cumulative_sum의 buckets_path에서는 누적 합산을 수행할 집계로 daily-total-quantity-average를 지정했습니다. 이렇게 지정하면 cumulative_sum을 수행할 때마다 daily-total-quantity-average를 찾아 그 합을 누적 합산합니다.

##### \[max_bucket 집계]
max_bucket은 다른 집계의 결과를 받아서 그 결과가 가장 큰 버킷의 key와 결과값을 구하는 집계입니다.

```
GET kibana_sample_data_ecommerce/_search
{
	"size": 0,
	"query": {
		"match_all": {}
	},
	"aggs": {
		"daily-timestamp-bucket": {
			"date_histogram": {
				"field": "order_date",
				"calendar_interval": "day"
			},
			"aggs": {
				"daily-total-quantity-average": {
					"avg": {
						"field": "total_quantity"
					}
				}
			}
		},
		"max-total_quantity": {
			"max_bucket": {
				"buckets_patch": "daily-timestamp-bucket>daily-total-quantity-average"
			}
		}
	}
}
```

요청을 자세히 보면 buckets_path 부분에 avg 집계의 이름인 daily-total-quantity-average 가 아니라 daily-timestamp-bucket>daily-total-quantity-average를 지정했다. 앞서 cumulative_sum 에서는 daily-total-quantity-average 만을 지정했었습니다.

값을 끌어올 집계 이름을 바로 명시하지 않은 이유는 buckets_path가 절대 경로가 아닌 상대 경로를 사용하기 때문입니다. 현재 max_bucket 집계가 위치한 계층에서는 daily-total-quantity-average를 인식할 수 없습니다. 따라서 daily-timestamp-bucket>daily-total-quantity-average로 한 단계씩 경로를 지정해야 의도한 대로 동작합니다.